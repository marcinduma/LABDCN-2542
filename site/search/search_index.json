{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Welcome to CiscoLive 2023 - Walk in Lab Speakers: Karol Okraska , CX Delivery Architect, Cisco Systems, Inc. Marcin Duma , CX Delivery Architect, Cisco Systems, Inc. Cisco ACI support of Public Cloud infrastructure Cisco Cloud Network Controller (formerly Cloud APIC) provides enterprises with networking tools necessary to accelerate their hybrid-cloud and/or multicloud journey. Utilizing cloud-native constructs, the solution enables automation that accelerates infrastructure deployment and governance and simplifies management to easily connect workloads across multicloud environments. The Cisco Cloud Network Controller vision is to support enhanced observability, operations, and troubleshooting across the entire environment. Cisco Cloud Network Controller enables: \u25cf Seamless connectivity for any workload at scale across any location \u25cf Operational simplicity and visibility across a vast multisite, multicloud \u25cf Data-center network \u25cf Easy L4-7 services integration \u25cf Consistent security and segmentation \u25cf Business continuity and disaster recovery Cisco Cloud Network Solution components: Cisco Cloud Network Controller is the main architectural component of this multicloud solution. It is the unified point of automation and management for the solution fabric including network and security policy, health monitoring, and optimizes performance and agility. The complete solution includes: \u25cf Cisco Cloud Network Controller ( called CNC later in labguide) (deployed in each Public Cloud which is to be managed) \u25cf Cisco Nexus Dashboard (in our lab deployed on-prem) - Multicloud networking orchestration and policy management, disaster recovery, and high availability, as well as provisioning and health monitoring. \u25cf Cisco Catalyst\u00ae 8000V - deployed in Public Clouds, allowing for communication with other Clouds or on-premises datacenter. Responsible for traffic secuirty and end to end policy enforcement. High Level Design of Lab scenario The Lab excercise is about showing you practical use of stretching services between on-prem and Public Cloud leveraging Cisco Nexus Dashboard Orchestrator and CNC for it. It gives you oportunity to check how easy can be maintaining your services in different places due to scalibility, costs, performance using Cisco Cloud products. Figure below is showing Logical design you are going to deploy in the Lab. Infrastructure part is already deployed, you don't need to bother with NDO installation, CNC installation or day-1 configs. This lab is solely to familiraze you with deployment of logical topology from the figure. Info Lab is based on Cisco ACI simulator, thus it won't be possible to run Data-plane between on-prem and AWS. If you are interested to deploy all infra in MultiCloud, please join our Instructor Led Lab LTRCLD-2557 . Lab diagram: As indicated in the diagram, this lab is using EMEA based regions in AWS as well as dCloud (onprem). POD runs on dedicated AWS infrastructure tenant which is already deployed and contains CNC and Cat8kv routers. Additionally each POD have dedicated AWS user tenant, its place where Policy model will be deployed. Onprem, Lab infrastructure contains Cisco Nexus Dashboard with Nexus Dashboard Orchestrator, Cisco ACI infrastructure and CSR1kv to terminate IPSec tunnels. Control plane infrastructure is already deployed for this lab. Use cases will be deployed in user tenant which needs to be onboarded in Cisco Network Controller and Nexus Dashboard Orchestrator. Enjoy!","title":"Home"},{"location":"index.html#welcome-to-ciscolive-2023-walk-in-lab","text":"Speakers: Karol Okraska , CX Delivery Architect, Cisco Systems, Inc. Marcin Duma , CX Delivery Architect, Cisco Systems, Inc.","title":"Welcome to CiscoLive 2023 - Walk in Lab"},{"location":"index.html#cisco-aci-support-of-public-cloud-infrastructure","text":"Cisco Cloud Network Controller (formerly Cloud APIC) provides enterprises with networking tools necessary to accelerate their hybrid-cloud and/or multicloud journey. Utilizing cloud-native constructs, the solution enables automation that accelerates infrastructure deployment and governance and simplifies management to easily connect workloads across multicloud environments. The Cisco Cloud Network Controller vision is to support enhanced observability, operations, and troubleshooting across the entire environment. Cisco Cloud Network Controller enables: \u25cf Seamless connectivity for any workload at scale across any location \u25cf Operational simplicity and visibility across a vast multisite, multicloud \u25cf Data-center network \u25cf Easy L4-7 services integration \u25cf Consistent security and segmentation \u25cf Business continuity and disaster recovery","title":"Cisco ACI support of Public Cloud infrastructure"},{"location":"index.html#cisco-cloud-network-solution-components","text":"Cisco Cloud Network Controller is the main architectural component of this multicloud solution. It is the unified point of automation and management for the solution fabric including network and security policy, health monitoring, and optimizes performance and agility. The complete solution includes: \u25cf Cisco Cloud Network Controller ( called CNC later in labguide) (deployed in each Public Cloud which is to be managed) \u25cf Cisco Nexus Dashboard (in our lab deployed on-prem) - Multicloud networking orchestration and policy management, disaster recovery, and high availability, as well as provisioning and health monitoring. \u25cf Cisco Catalyst\u00ae 8000V - deployed in Public Clouds, allowing for communication with other Clouds or on-premises datacenter. Responsible for traffic secuirty and end to end policy enforcement.","title":"Cisco Cloud Network Solution components:"},{"location":"index.html#high-level-design-of-lab-scenario","text":"The Lab excercise is about showing you practical use of stretching services between on-prem and Public Cloud leveraging Cisco Nexus Dashboard Orchestrator and CNC for it. It gives you oportunity to check how easy can be maintaining your services in different places due to scalibility, costs, performance using Cisco Cloud products. Figure below is showing Logical design you are going to deploy in the Lab. Infrastructure part is already deployed, you don't need to bother with NDO installation, CNC installation or day-1 configs. This lab is solely to familiraze you with deployment of logical topology from the figure. Info Lab is based on Cisco ACI simulator, thus it won't be possible to run Data-plane between on-prem and AWS. If you are interested to deploy all infra in MultiCloud, please join our Instructor Led Lab LTRCLD-2557 . Lab diagram: As indicated in the diagram, this lab is using EMEA based regions in AWS as well as dCloud (onprem). POD runs on dedicated AWS infrastructure tenant which is already deployed and contains CNC and Cat8kv routers. Additionally each POD have dedicated AWS user tenant, its place where Policy model will be deployed. Onprem, Lab infrastructure contains Cisco Nexus Dashboard with Nexus Dashboard Orchestrator, Cisco ACI infrastructure and CSR1kv to terminate IPSec tunnels. Control plane infrastructure is already deployed for this lab. Use cases will be deployed in user tenant which needs to be onboarded in Cisco Network Controller and Nexus Dashboard Orchestrator. Enjoy!","title":"High Level Design of Lab scenario"},{"location":"LAB_access-RDP.html","text":"Lab access 1. Lab access general description The lab has been built leveraging two environments as following: Amazon Web Services Private intrastructure on-prem You will have access to Cisco dCloud Workstation, where you will be able to open WEB GUI to: Cisco Nexus Dashboard/NDO onprem Cisco APIC onprem Cisco Network Controller (CNC) - hosted in AWS 2. VPN connection to dCloud infrastructure The entire lab for the session is built using Cisco dCloud environment. Details of the session are provided in POD assigned to you. You will find there \"Connect VPN\" link which allow you connection to dCloud session. When you decide to use BYOD, please connect to VPN using provided credentials in WIL assistant webpage. Access session with RDP Once logged to the dCloud session, you will be able to RDP Windows workstation ready to use for you Open RDP client located on Windows Desktop (if you are using BYOD locate it in your OS) and use credentials provided below: IP Address: 198.18.133.36 Username: dcloud\\demouser User password: C1sco12345 When you login to Remote Desktop, you will find installed Chrome as web browser, from where you get access to GUI resources. 1. Accessing Cisco Nexus Dashboard/NDO Once you are connected to RDP workstation, please open Chrome and by using Bookmark bar select Nexus Dashboard . IP Address: 198.18.134.200 Username: admin User password: C1sco12345 2. Accessing Cisco APIC Once you are connected to RDP workstation, please open Chrome and by using Bookmark bar select APIC . IP Address: 198.18.133.200 Username: admin User password: C1sco12345 3. Accessing Cisco Cloud Network Controller (CNC) IP Address of your CNC will be provided during the lab in later stage. Each Pod has its own CNC, please open Chrome and by using Bookmark bar select Cloud Network Controller . Credentials to login are same for all of them: Username: admin User password: CiscoLive2@23! 4. Accessing AWS Console Your access to AWS User tenant per POD AWS console login link: https://console.aws.amazon.com Information about account details as well as credentials are available in per POD sections listed in wilassistant.","title":"Lab Access"},{"location":"LAB_access-RDP.html#lab-access","text":"","title":"Lab access"},{"location":"LAB_access-RDP.html#1-lab-access-general-description","text":"The lab has been built leveraging two environments as following: Amazon Web Services Private intrastructure on-prem You will have access to Cisco dCloud Workstation, where you will be able to open WEB GUI to: Cisco Nexus Dashboard/NDO onprem Cisco APIC onprem Cisco Network Controller (CNC) - hosted in AWS","title":"1. Lab access general description"},{"location":"LAB_access-RDP.html#2-vpn-connection-to-dcloud-infrastructure","text":"The entire lab for the session is built using Cisco dCloud environment. Details of the session are provided in POD assigned to you. You will find there \"Connect VPN\" link which allow you connection to dCloud session. When you decide to use BYOD, please connect to VPN using provided credentials in WIL assistant webpage.","title":"2. VPN connection to dCloud infrastructure"},{"location":"LAB_access-RDP.html#access-session-with-rdp","text":"Once logged to the dCloud session, you will be able to RDP Windows workstation ready to use for you Open RDP client located on Windows Desktop (if you are using BYOD locate it in your OS) and use credentials provided below: IP Address: 198.18.133.36 Username: dcloud\\demouser User password: C1sco12345 When you login to Remote Desktop, you will find installed Chrome as web browser, from where you get access to GUI resources.","title":"Access session with RDP"},{"location":"LAB_access-RDP.html#1-accessing-cisco-nexus-dashboardndo","text":"Once you are connected to RDP workstation, please open Chrome and by using Bookmark bar select Nexus Dashboard . IP Address: 198.18.134.200 Username: admin User password: C1sco12345","title":"1. Accessing Cisco Nexus Dashboard/NDO"},{"location":"LAB_access-RDP.html#2-accessing-cisco-apic","text":"Once you are connected to RDP workstation, please open Chrome and by using Bookmark bar select APIC . IP Address: 198.18.133.200 Username: admin User password: C1sco12345","title":"2. Accessing Cisco APIC"},{"location":"LAB_access-RDP.html#3-accessing-cisco-cloud-network-controller-cnc","text":"IP Address of your CNC will be provided during the lab in later stage. Each Pod has its own CNC, please open Chrome and by using Bookmark bar select Cloud Network Controller . Credentials to login are same for all of them: Username: admin User password: CiscoLive2@23!","title":"3. Accessing Cisco Cloud Network Controller (CNC)"},{"location":"LAB_access-RDP.html#4-accessing-aws-console","text":"Your access to AWS User tenant per POD AWS console login link: https://console.aws.amazon.com Information about account details as well as credentials are available in per POD sections listed in wilassistant.","title":"4. Accessing AWS Console"},{"location":"LAB_access.html","text":"Connectivity Check 1. Lab access general description This lab is fully hosted in Public Cloud infrastrucure, both AWS and Azure, no local resources are used. You will have access to Public Cloud Console Interface, as well as you will be able to login to necessary resources and appliances. 2. AWS and Azure console access details: Login link for AWS console: https://console.aws.amazon.com Login link for Azure portal: https://portal.azure.com 3. Accessing Cloud Network Controller and Nexus Dashboard. All appliances are addressed with Public IP address, hence it can be accessed from local browser without any additional settings or VPN connections. List of IP addreses for each POD can be found in respective POD link: Lab Details POD1 Lab Details POD2","title":"Connectivity Check"},{"location":"LAB_access.html#connectivity-check","text":"","title":"Connectivity Check"},{"location":"LAB_access.html#1-lab-access-general-description","text":"This lab is fully hosted in Public Cloud infrastrucure, both AWS and Azure, no local resources are used. You will have access to Public Cloud Console Interface, as well as you will be able to login to necessary resources and appliances.","title":"1. Lab access general description"},{"location":"LAB_access.html#2-aws-and-azure-console-access-details","text":"Login link for AWS console: https://console.aws.amazon.com Login link for Azure portal: https://portal.azure.com","title":"2. AWS and Azure console access details:"},{"location":"LAB_access.html#3-accessing-cloud-network-controller-and-nexus-dashboard","text":"All appliances are addressed with Public IP address, hence it can be accessed from local browser without any additional settings or VPN connections. List of IP addreses for each POD can be found in respective POD link: Lab Details POD1 Lab Details POD2","title":"3. Accessing Cloud Network Controller and Nexus Dashboard."},{"location":"aws-trust.html","text":"Nexus Dasboard Orchestrator Tenant AWS Trust Configuration In previous step you have selected Tenant configuration Mode as Trusted, so now trust needs to be made for Cloud Network Controller to have access to AWS Account. AWS Cloud Network Controller Login Using the IP of AWS Cisco Cloud Network Controller (can be found in Chrome bookmarks), connect to CNC GUI for AWS instance. Provide Credentials and hit Login Username: admin Password: CiscoLive2@23! Hit \"Get started\" to view Cloud Network Controller Dashboard. Look on the Dashboard -> Application Management -> Tenants , we can see newly create tenant on the list. Double-click the Tenant name \"Tenant-01\" to open it. Under the AWS Account section there is warning related to our Trust configuration, along with the \"Run the CloudFormation Template\" hyperlink. Click on the hyperlink to run CloudFormation Script. You will be then redirected to AWS console CloudFormation Stack configuration. In this time you will be asked to login go AWS console if not done before. If already loged in, please continue to CloudFormation script in AWS . AWS User Account Login Open AWS console via browser https://console.aws.amazon.com Select IAM user, provide Account ID and hit \"Next\" Note For this login please use AWS User Account ID from POD details - Section Lab Access . Use data from POD# assigned to you in Wil Assistant portal or ask proctor for help. Provide Username and password and hit \"Sign In\" CloudFormation script in AWS All the parameters are already filled in, hit \"Next\" to continue. In Specify stack details step - fill in stack name and hit \"Next\" Stack Name: CNC-AWS-Trust Parameters: none In Configure stack options step - leave all options as default and hit \"Next\" In Review CNC-AWS-Trust step - leave all options as default, scroll down to \"Capabilities\" section and check the checkbox for resource creation and hit \"Submit\" After couple of seconds(use refresh button) - stack will be completed and we are good to go. After AWS trust configuration you have your first Tenant ready to be used. In next section of the Lab you will procced to Use-Cases configuration. Two(2) use-cases you are about to configure are presenting typical real-life scenearios.","title":"AWS Trust"},{"location":"aws-trust.html#nexus-dasboard-orchestrator-tenant-aws-trust-configuration","text":"In previous step you have selected Tenant configuration Mode as Trusted, so now trust needs to be made for Cloud Network Controller to have access to AWS Account.","title":"Nexus Dasboard Orchestrator Tenant AWS Trust Configuration"},{"location":"aws-trust.html#aws-cloud-network-controller-login","text":"Using the IP of AWS Cisco Cloud Network Controller (can be found in Chrome bookmarks), connect to CNC GUI for AWS instance. Provide Credentials and hit Login Username: admin Password: CiscoLive2@23! Hit \"Get started\" to view Cloud Network Controller Dashboard. Look on the Dashboard -> Application Management -> Tenants , we can see newly create tenant on the list. Double-click the Tenant name \"Tenant-01\" to open it. Under the AWS Account section there is warning related to our Trust configuration, along with the \"Run the CloudFormation Template\" hyperlink. Click on the hyperlink to run CloudFormation Script. You will be then redirected to AWS console CloudFormation Stack configuration. In this time you will be asked to login go AWS console if not done before. If already loged in, please continue to CloudFormation script in AWS .","title":"AWS Cloud Network Controller Login"},{"location":"aws-trust.html#aws-user-account-login","text":"Open AWS console via browser https://console.aws.amazon.com Select IAM user, provide Account ID and hit \"Next\" Note For this login please use AWS User Account ID from POD details - Section Lab Access . Use data from POD# assigned to you in Wil Assistant portal or ask proctor for help. Provide Username and password and hit \"Sign In\"","title":"AWS User Account Login"},{"location":"aws-trust.html#cloudformation-script-in-aws","text":"All the parameters are already filled in, hit \"Next\" to continue. In Specify stack details step - fill in stack name and hit \"Next\" Stack Name: CNC-AWS-Trust Parameters: none In Configure stack options step - leave all options as default and hit \"Next\" In Review CNC-AWS-Trust step - leave all options as default, scroll down to \"Capabilities\" section and check the checkbox for resource creation and hit \"Submit\" After couple of seconds(use refresh button) - stack will be completed and we are good to go. After AWS trust configuration you have your first Tenant ready to be used. In next section of the Lab you will procced to Use-Cases configuration. Two(2) use-cases you are about to configure are presenting typical real-life scenearios.","title":"CloudFormation script in AWS"},{"location":"guacamole.html","text":"Copy and Paste in and out Guacamole Apache Guacamole serves as a proxy to provide clientless access to other servers. The copy paste function is achieved through browser. Guacamole Menu The guacamole menu is a sidebar which is hidden. On a Windows device with an external keyboard, the guacamole menu is displayed by pressing Ctrl+Alt+Shift On a Mac device with an external keyboard, the guacamole menu is displayed by pressing Ctrl+Command+Shift 3. On a device that doesn\u2019t have a keyboard, e.g. mobile or touchscreen device, the guacamole menu is displayed by swiping right from the left edge of the screen. To Hide the guacamole menu, press Ctrl+Alt+Shift/ Ctrl+Command+Shift or swipe left across the screen again. Guacamole Use The Clipboard text area functions as an interface between the remote clipboard and local clipboard. Text from the local clipboard can be pasted into the text area, and the text is sent to the remote clipboard. 2. After the text is copied to the remote clipboard, the Guacamole menu can be closed. The text from the clipboard can be pasted in the desired location. 3. Similarly, copying text from the remote desktop sends the text to the Clipboard text area on the Guacamole Menu, which can then be copied to the local clipboard.","title":"Copy and Paste in and out Guacamole"},{"location":"guacamole.html#copy-and-paste-in-and-out-guacamole","text":"Apache Guacamole serves as a proxy to provide clientless access to other servers. The copy paste function is achieved through browser.","title":"Copy and Paste in and out Guacamole"},{"location":"guacamole.html#guacamole-menu","text":"The guacamole menu is a sidebar which is hidden. On a Windows device with an external keyboard, the guacamole menu is displayed by pressing Ctrl+Alt+Shift On a Mac device with an external keyboard, the guacamole menu is displayed by pressing Ctrl+Command+Shift 3. On a device that doesn\u2019t have a keyboard, e.g. mobile or touchscreen device, the guacamole menu is displayed by swiping right from the left edge of the screen. To Hide the guacamole menu, press Ctrl+Alt+Shift/ Ctrl+Command+Shift or swipe left across the screen again.","title":"Guacamole Menu"},{"location":"guacamole.html#guacamole-use","text":"The Clipboard text area functions as an interface between the remote clipboard and local clipboard. Text from the local clipboard can be pasted into the text area, and the text is sent to the remote clipboard. 2. After the text is copied to the remote clipboard, the Guacamole menu can be closed. The text from the clipboard can be pasted in the desired location. 3. Similarly, copying text from the remote desktop sends the text to the Clipboard text area on the Guacamole Menu, which can then be copied to the local clipboard.","title":"Guacamole Use"},{"location":"infra-config-check.html","text":"","title":"Infra config check"},{"location":"infra-config-msite.html","text":"Infrastructure configuration - MultiSite configuration In this lab section we will use Nexus Dashboard Orchestrator(NDO) to connect 2 Cloud Fabrics together. Nexus Dashboard Orchestrator(NDO) Cisco Nexus Dashboard Orchestrator (NDO) provides consistent network and policy orchestration, scalability, and disaster recovery across multiple data centers through a single pane of glass while allowing the data center to go wherever the data is. NDO allows you to interconnect separate Cisco\u00ae Application Centric Infrastructure (Cisco ACI\u00ae) sites, Cisco Cloud ACI sites, and Cisco Nexus Dashboard Fabric Controller (NDFC) sites, each managed by its own controller (APIC cluster, NDFC cluster, or Cloud APIC instances in a public cloud). The on-premises sites (ACI or NDFC in the future) can be extended to different public clouds for hybrid-cloud deployments while cloud-first installations can be extended to multi-cloud deployments without on-premises sites. The single-pane network interconnect policy management and the consistent network workload and segmentation policy provided by NDO allows monitoring the health of the interconnected fabrics, enforcement of segmentation and security policies, and performance of all tasks required to define tenant intersite policies in APIC, NDFC, and cAPIC sites. \u25cf Key features and benefits Single pane of glass for administration and orchestration of multiple networking fabrics for both Cisco ACI and NDFC Automation of the configuration and management of intersite network interconnects across an IP backbone for both Cisco ACI and NDFC Consistent multitenant policy across multiple sites, which allows IP mobility, disaster recovery, and active/active use cases for data centers Capability to map tenants, applications, and associated networks to specific availability domains within the Cisco Multi-Site architecture for both Cisco ACI and NDFC Hybrid cloud and multi-cloud orchestration supporting on-premises Cisco ACI sites and public cloud sites (AWS and Azure) Capability to have multi-cloud ACI deployments without on-premises sites Scale out sites and leaf switches based on resource growth \u25cfUse cases There are several uses of Cisco Nexus Dashboard Orchestrator. Some of the main use cases include: Large scale data center deployment Data center interconnectivity Cisco NDO multi-domain integrations Hybrid cloud and multi-cloud Service provider \u2013 5G telco Let's now explore possibilites of Cisco Nexus Dashboard Orchestrator! 1. NDO Service Introduction In the navigation menu on the left go to \"Admin Console\" You will be moved to Admin Console page, where you can see Health of your Nexus Dashboard and status of sites and services. Navigate to \"Sites\" tab and verify if \"Connectivity Status\" of you sites is \"Up\" Navigate to \"Sevices\" tab to see what applications are installed on this Nexus Dashboard Nexus Dashboard Orchestrator is already installed and enabled under \"Service Catalog\" inside \"Installed Service\" Hit \"Open\" button in order to login You will be now redirected to Nexus Dashboard Orchestrator Dashboard. 2. NDO Sites Onboarding In order to configure AWS and Azure sites from NDO, added previously on Nexus Dashboard , sites have to be Maneged and have Site ID assigned. Navigate to Sites : Click on \"Unmanaged\" box under State Column for each site and assign Site ID , confirming with \"Add\" button. For AWS Site - set ID **10** For Azure Site - set ID **20** After this operation - both sites should be visible as Managed 3. Site Connectivity Configuration In Next Step we would configure Infrastructure to connect 2 Cloud ACI Sites togehter. Navigate to Infrastructure -> Site Connectivity -> Configure button On the General Settings Page, scroll down to OSPF Configuration and fill in OSPF Area ID to value 0.0.0.0 , leave other setting as default. OSPF Protocol will be configured over IPSec tunnel and allows for prefixes exchange to bring BGP Neighbourship. Go to tab \"IPSec Tunnel Subnet Pools\" , click Add IP address button and add subnet Subnet: 192.168.255.0/24 Confirm with checkbox button. You may notice that there is already one subnet 169.254.0.0/16 configured, which is used for Tunnel addresing between Cloud Routers and Cloud Load Balancers(Azure)/Transit Gateways(AWS). Those tunnles are used to forward traffic from Cloud instances to Cloud Routers and further to another sites. In the left navigation bar, under the Sites bar, click on first site CNC-AWS-01 Enable the site for MultiSite by checking the checkbox \"ACI Multisite\" also enable \"Contract Based Routing\" Settings: - ACI Multisite - checked - Contract Based Routing - checked Click the \"Add Site\" button to cross-connect 2 sites. Under Connected to Site select Select a Site hyperlink. Select Azure fabric and hit Select For the Connection Type select Public Internet and hit Ok . Leave rest of the setting as default, you can review them. Move to CNC-Azure-01 Site and also enable the site for MultiSite by checking the checkbox \"ACI Multisite\" - simiar as in previous point. Settings: - ACI Multisite - checked - Contract Based Routing - checked Note that second site is already selected for Inter-Site Connectivity Once done, locate the Deploy button on top the screen, click it and Select \"Deploy Only\" , hit Yes for confirmation. As you hit Deploy button, NDO will now configure CNC and Cloud Routes (previously deployed and visible in AWS EC2/Azure Virutal Machines pages). Configuration includes: - IPSec tunnels, full mesh between all 4 routers (2 per Cloud) - OSPF routing over IPSec Tunnels - BGP EVPN peering for prefixes exchange Inter-site connectivity veryfication It may take 5-10 minutes for configuration to be pushed and Tunnels to be established. At this point we configured this part of our topology diagram: 1. Nexush Dashboard view Nexus Dashboard allows for monitoring of Inter-Site connectivity. On the Left navigation page click \"Dashboard\" to go back to main Connectivity View. Take a look into green line between AWS and Azure site(you can use use magnifying tool for better view). Green line indicated that all is fine with connectivity. On the Left navigation page click \"Infrastructure\" -> \"Site Connectivity\" and scroll down on a page. Under the site list, locate \"Show Connectivity Status\" and click on it. Check the connectivity status for both BGP EVPN as well as Tunnel Status. There should be 4 UP BGP sessions, as well as 4 Tunnels which are UP between Sites. Inter-site connectivity veryfication (Cloud Routers) 1. IPSec tunnel veryfication Open Putty client from desktop and put IP address of Cloud router. Note Cloud Routers IP address are avaibale in POD details IP address schema. Login with username \"csradmin\" and password \"CiscoLive2023!\" Execute command \"show ip int brief | include Tunnel\" show ip int brief | include Tunnel Expected output: ct_routerp_eu-central-1_0_0#show ip int brief | include Tunnel Tunnel0 10.10.0.52 YES unset up up Tunnel1 169.254.112.1 YES NVRAM up up Tunnel6 192.168.255.4 YES NVRAM up up Tunnel7 192.168.255.2 YES NVRAM up up ct_routerp_eu-central-1_0_0# Verify that all 4 tunnels are up/up. Tunnel0 is addressed from TEP pool of our Cloud Contoller Fabric and it's used for communication between CNC and Cloud Router. Tunnel1 is a connected to inside of a Cloud. Depends on the provider - it's an Azure Load Balancer or AWS Transit Gateway and routing will be injected to send traffic towards VNETs and VPC CIDRs. Tunnel6 and Tunnel7 are configured towards two (2) Catalyst 8000V Routers in another site. Note also that Tunnel6 and Tunnel7 are addressed from IPSec subnet which was specified in the beginning of Multiste Configuration. 2. OSPF adjacency veryfication Execute command \"show ip ospf neighbor\" show ip ospf neighbor Expected output: ct_routerp_eu-central-1_0_0#show ip ospf neighbor Neighbor ID Pri State Dead Time Address Interface 10.20.0.20 0 FULL/ - 00:00:33 192.168.255.3 Tunnel7 10.20.0.68 0 FULL/ - 00:00:38 192.168.255.5 Tunnel6 ct_routerp_eu-central-1_0_0# Verify that both sessions are in FULL State. OSPF Neighbourship allows for prefixes exchange, so BGP session can be build. 2. BGP EVPN veryfication Execute command \"show bgp l2vpn evpn summary\" show bgp l2vpn evpn summary Expected output: ct_routerp_eu-central-1_0_0#show bgp l2vpn evpn summary BGP router identifier 10.10.0.20, local AS number 65110 BGP table version is 1, main routing table version 1 Neighbor V AS MsgRcvd MsgSent TblVer InQ OutQ Up/Down State/PfxRcd 10.20.0.52 4 65200 96 94 1 0 0 01:22:47 0 10.20.0.116 4 65200 95 94 1 0 0 01:22:52 0 ct_routerp_eu-central-1_0_0# Verify that both sessions are Up (O in \"State/PfxRcd\" column) In Next Section our First Tenant will be configured!","title":"Infrastructure configuration - MultiSite configuration"},{"location":"infra-config-msite.html#infrastructure-configuration-multisite-configuration","text":"In this lab section we will use Nexus Dashboard Orchestrator(NDO) to connect 2 Cloud Fabrics together.","title":"Infrastructure configuration - MultiSite configuration"},{"location":"infra-config-msite.html#nexus-dashboard-orchestratorndo","text":"Cisco Nexus Dashboard Orchestrator (NDO) provides consistent network and policy orchestration, scalability, and disaster recovery across multiple data centers through a single pane of glass while allowing the data center to go wherever the data is. NDO allows you to interconnect separate Cisco\u00ae Application Centric Infrastructure (Cisco ACI\u00ae) sites, Cisco Cloud ACI sites, and Cisco Nexus Dashboard Fabric Controller (NDFC) sites, each managed by its own controller (APIC cluster, NDFC cluster, or Cloud APIC instances in a public cloud). The on-premises sites (ACI or NDFC in the future) can be extended to different public clouds for hybrid-cloud deployments while cloud-first installations can be extended to multi-cloud deployments without on-premises sites. The single-pane network interconnect policy management and the consistent network workload and segmentation policy provided by NDO allows monitoring the health of the interconnected fabrics, enforcement of segmentation and security policies, and performance of all tasks required to define tenant intersite policies in APIC, NDFC, and cAPIC sites. \u25cf Key features and benefits Single pane of glass for administration and orchestration of multiple networking fabrics for both Cisco ACI and NDFC Automation of the configuration and management of intersite network interconnects across an IP backbone for both Cisco ACI and NDFC Consistent multitenant policy across multiple sites, which allows IP mobility, disaster recovery, and active/active use cases for data centers Capability to map tenants, applications, and associated networks to specific availability domains within the Cisco Multi-Site architecture for both Cisco ACI and NDFC Hybrid cloud and multi-cloud orchestration supporting on-premises Cisco ACI sites and public cloud sites (AWS and Azure) Capability to have multi-cloud ACI deployments without on-premises sites Scale out sites and leaf switches based on resource growth \u25cfUse cases There are several uses of Cisco Nexus Dashboard Orchestrator. Some of the main use cases include: Large scale data center deployment Data center interconnectivity Cisco NDO multi-domain integrations Hybrid cloud and multi-cloud Service provider \u2013 5G telco Let's now explore possibilites of Cisco Nexus Dashboard Orchestrator!","title":"Nexus Dashboard Orchestrator(NDO)"},{"location":"infra-config-msite.html#1-ndo-service-introduction","text":"In the navigation menu on the left go to \"Admin Console\" You will be moved to Admin Console page, where you can see Health of your Nexus Dashboard and status of sites and services. Navigate to \"Sites\" tab and verify if \"Connectivity Status\" of you sites is \"Up\" Navigate to \"Sevices\" tab to see what applications are installed on this Nexus Dashboard Nexus Dashboard Orchestrator is already installed and enabled under \"Service Catalog\" inside \"Installed Service\" Hit \"Open\" button in order to login You will be now redirected to Nexus Dashboard Orchestrator Dashboard.","title":"1. NDO Service Introduction"},{"location":"infra-config-msite.html#2-ndo-sites-onboarding","text":"In order to configure AWS and Azure sites from NDO, added previously on Nexus Dashboard , sites have to be Maneged and have Site ID assigned. Navigate to Sites : Click on \"Unmanaged\" box under State Column for each site and assign Site ID , confirming with \"Add\" button. For AWS Site - set ID **10** For Azure Site - set ID **20** After this operation - both sites should be visible as Managed","title":"2. NDO Sites Onboarding"},{"location":"infra-config-msite.html#3-site-connectivity-configuration","text":"In Next Step we would configure Infrastructure to connect 2 Cloud ACI Sites togehter. Navigate to Infrastructure -> Site Connectivity -> Configure button On the General Settings Page, scroll down to OSPF Configuration and fill in OSPF Area ID to value 0.0.0.0 , leave other setting as default. OSPF Protocol will be configured over IPSec tunnel and allows for prefixes exchange to bring BGP Neighbourship. Go to tab \"IPSec Tunnel Subnet Pools\" , click Add IP address button and add subnet Subnet: 192.168.255.0/24 Confirm with checkbox button. You may notice that there is already one subnet 169.254.0.0/16 configured, which is used for Tunnel addresing between Cloud Routers and Cloud Load Balancers(Azure)/Transit Gateways(AWS). Those tunnles are used to forward traffic from Cloud instances to Cloud Routers and further to another sites. In the left navigation bar, under the Sites bar, click on first site CNC-AWS-01 Enable the site for MultiSite by checking the checkbox \"ACI Multisite\" also enable \"Contract Based Routing\" Settings: - ACI Multisite - checked - Contract Based Routing - checked Click the \"Add Site\" button to cross-connect 2 sites. Under Connected to Site select Select a Site hyperlink. Select Azure fabric and hit Select For the Connection Type select Public Internet and hit Ok . Leave rest of the setting as default, you can review them. Move to CNC-Azure-01 Site and also enable the site for MultiSite by checking the checkbox \"ACI Multisite\" - simiar as in previous point. Settings: - ACI Multisite - checked - Contract Based Routing - checked Note that second site is already selected for Inter-Site Connectivity Once done, locate the Deploy button on top the screen, click it and Select \"Deploy Only\" , hit Yes for confirmation. As you hit Deploy button, NDO will now configure CNC and Cloud Routes (previously deployed and visible in AWS EC2/Azure Virutal Machines pages). Configuration includes: - IPSec tunnels, full mesh between all 4 routers (2 per Cloud) - OSPF routing over IPSec Tunnels - BGP EVPN peering for prefixes exchange","title":"3. Site Connectivity Configuration"},{"location":"infra-config-msite.html#inter-site-connectivity-veryfication","text":"It may take 5-10 minutes for configuration to be pushed and Tunnels to be established. At this point we configured this part of our topology diagram:","title":"Inter-site connectivity veryfication"},{"location":"infra-config-msite.html#1-nexush-dashboard-view","text":"Nexus Dashboard allows for monitoring of Inter-Site connectivity. On the Left navigation page click \"Dashboard\" to go back to main Connectivity View. Take a look into green line between AWS and Azure site(you can use use magnifying tool for better view). Green line indicated that all is fine with connectivity. On the Left navigation page click \"Infrastructure\" -> \"Site Connectivity\" and scroll down on a page. Under the site list, locate \"Show Connectivity Status\" and click on it. Check the connectivity status for both BGP EVPN as well as Tunnel Status. There should be 4 UP BGP sessions, as well as 4 Tunnels which are UP between Sites.","title":"1. Nexush Dashboard view"},{"location":"infra-config-msite.html#inter-site-connectivity-veryfication-cloud-routers","text":"","title":"Inter-site connectivity veryfication  (Cloud Routers)"},{"location":"infra-config-msite.html#1-ipsec-tunnel-veryfication","text":"Open Putty client from desktop and put IP address of Cloud router. Note Cloud Routers IP address are avaibale in POD details IP address schema. Login with username \"csradmin\" and password \"CiscoLive2023!\" Execute command \"show ip int brief | include Tunnel\" show ip int brief | include Tunnel Expected output: ct_routerp_eu-central-1_0_0#show ip int brief | include Tunnel Tunnel0 10.10.0.52 YES unset up up Tunnel1 169.254.112.1 YES NVRAM up up Tunnel6 192.168.255.4 YES NVRAM up up Tunnel7 192.168.255.2 YES NVRAM up up ct_routerp_eu-central-1_0_0# Verify that all 4 tunnels are up/up. Tunnel0 is addressed from TEP pool of our Cloud Contoller Fabric and it's used for communication between CNC and Cloud Router. Tunnel1 is a connected to inside of a Cloud. Depends on the provider - it's an Azure Load Balancer or AWS Transit Gateway and routing will be injected to send traffic towards VNETs and VPC CIDRs. Tunnel6 and Tunnel7 are configured towards two (2) Catalyst 8000V Routers in another site. Note also that Tunnel6 and Tunnel7 are addressed from IPSec subnet which was specified in the beginning of Multiste Configuration.","title":"1. IPSec tunnel veryfication"},{"location":"infra-config-msite.html#2-ospf-adjacency-veryfication","text":"Execute command \"show ip ospf neighbor\" show ip ospf neighbor Expected output: ct_routerp_eu-central-1_0_0#show ip ospf neighbor Neighbor ID Pri State Dead Time Address Interface 10.20.0.20 0 FULL/ - 00:00:33 192.168.255.3 Tunnel7 10.20.0.68 0 FULL/ - 00:00:38 192.168.255.5 Tunnel6 ct_routerp_eu-central-1_0_0# Verify that both sessions are in FULL State. OSPF Neighbourship allows for prefixes exchange, so BGP session can be build.","title":"2. OSPF adjacency veryfication"},{"location":"infra-config-msite.html#2-bgp-evpn-veryfication","text":"Execute command \"show bgp l2vpn evpn summary\" show bgp l2vpn evpn summary Expected output: ct_routerp_eu-central-1_0_0#show bgp l2vpn evpn summary BGP router identifier 10.10.0.20, local AS number 65110 BGP table version is 1, main routing table version 1 Neighbor V AS MsgRcvd MsgSent TblVer InQ OutQ Up/Down State/PfxRcd 10.20.0.52 4 65200 96 94 1 0 0 01:22:47 0 10.20.0.116 4 65200 95 94 1 0 0 01:22:52 0 ct_routerp_eu-central-1_0_0# Verify that both sessions are Up (O in \"State/PfxRcd\" column) In Next Section our First Tenant will be configured!","title":"2. BGP EVPN veryfication"},{"location":"infra-config.html","text":"Site Connectivity Configuration Next Step will be verification of our Infrastructure connectivity - Configuration changes are not necessary. Navigate to Infrastructure -> Site Connectivity -> Configure button On the General Settings Page, normally you will configure your global settings for entire MultiSite interconnection. Here you have BGP as well as OSPF polices, which will be deployed to every Site specified on the Left. General Settings section is divided to four different tabs. Each of tab is important to fill with correct data. IPN devices and External devices tabs contains information about IPN onprem as well as Cloud Cat8k devices respectively. Those informations are then used to creation of configuration to be deployed on onprem IPN devices and Cat8kv. At last tab - IPSec Tunnel Subnet pools, contains details about Site to Site connection types. Go to tab \"IPSec Tunnel Subnet Pools\" , to specify your IPSec details for securing communication across public Internet. Info You may notice that there is already one subnet 169.254.0.0/16 configured, which is used for Tunnel addresing between Cloud Routers and Cloud Load Balancers(Azure)/Transit Gateways(AWS). Those tunnles are used to forward traffic from Cloud instances to Cloud Routers and further to another sites. Second subnet 192.168.255.0/24 is used to address Tunnel interfaces between Sites in our case onpmrem CSR1kv router and Cat8kv routers in AWS. In the left navigation bar, under the Sites bar, click on first site cAPIC You can verify site configuration on right side of the screen. Those information needs to be there. Settings: - ACI Multisite - checked - Contract Based Routing - checked - Inter-Site connectivity - on-prem selected (BGP-EVPN) - Connection Type: Public On-prem site have more information to define. All of them are deployed already to the site. Info You can explore rest of the Site connectivity sections by yourself or move to Logical configuration on the next page. In Next Section our First Tenant will be configured!","title":"Sites connectivity"},{"location":"infra-config.html#site-connectivity-configuration","text":"Next Step will be verification of our Infrastructure connectivity - Configuration changes are not necessary. Navigate to Infrastructure -> Site Connectivity -> Configure button On the General Settings Page, normally you will configure your global settings for entire MultiSite interconnection. Here you have BGP as well as OSPF polices, which will be deployed to every Site specified on the Left. General Settings section is divided to four different tabs. Each of tab is important to fill with correct data. IPN devices and External devices tabs contains information about IPN onprem as well as Cloud Cat8k devices respectively. Those informations are then used to creation of configuration to be deployed on onprem IPN devices and Cat8kv. At last tab - IPSec Tunnel Subnet pools, contains details about Site to Site connection types. Go to tab \"IPSec Tunnel Subnet Pools\" , to specify your IPSec details for securing communication across public Internet. Info You may notice that there is already one subnet 169.254.0.0/16 configured, which is used for Tunnel addresing between Cloud Routers and Cloud Load Balancers(Azure)/Transit Gateways(AWS). Those tunnles are used to forward traffic from Cloud instances to Cloud Routers and further to another sites. Second subnet 192.168.255.0/24 is used to address Tunnel interfaces between Sites in our case onpmrem CSR1kv router and Cat8kv routers in AWS. In the left navigation bar, under the Sites bar, click on first site cAPIC You can verify site configuration on right side of the screen. Those information needs to be there. Settings: - ACI Multisite - checked - Contract Based Routing - checked - Inter-Site connectivity - on-prem selected (BGP-EVPN) - Connection Type: Public On-prem site have more information to define. All of them are deployed already to the site. Info You can explore rest of the Site connectivity sections by yourself or move to Logical configuration on the next page. In Next Section our First Tenant will be configured!","title":"Site Connectivity Configuration"},{"location":"intersight.html","text":"Explore Cisco Intersight Dashboard 1. Accessing Cisco Intersight Platform Cisco Intersight Platform manages Kuberenetes clusters in the private infrasturcture. You will have access to dedicated instance of Cisco Intersight, from which you will manage your own Kuberenetes Clusters used later on to deploy application. Please find login credentials and URL to your IKS instance below: URL: https://intersight.com/ User name: holcld2611+pod'X'@gmail.com Where X will be provided by proctor User password: CiscoLive!2022 Warning You can explore Cisco Intersight through the GUI, but please do not delete content already created. Access Intersight using following steps: Open WebBrowser (Chrome) and connect to https://intersight.com Select Sign In with Cisco ID option on the first screen - as shown on figure below. Enter e-mail provided by proctor to your POD: Click next and fill password. Tip Credentials for the session can be found in WIL Assistant portal. In case of issues, ask proctor for help. 2. Intersigh Dashboard Target While you are logged to Cisco Intersight, first page you see is Monitor section. Navigate to Admin section in the bottom of Navigation Pane on Left. Click on Target Target section shows already Claimed services - in our case it is Cisco Intersight Assist installed \"on-prem\" together with vCenter. You can see status of claim, date and who executed it. Click on our Intersight Assist - iva.dcloud.cisco.com to see details of the Target. 3. Intersight Dashboard Operate Most information from claimed targets are visible under Operate section. The section is divided to two sub categories: Virtualization and Kuberenetes. Virtualization When exloring first of them, you will see information about all VMs managed by vCenter claimed in Target section. By using three dots button on right column, you are able to manage each of VM: Option when open the Menu : On the other Tabs, you can verify details of Hosts, Storage, etc. Please explore yourself rest of the tab when interested. Kuberenetes Second section contain informations about deployed Intersight Kuberenetes Clusters (IKS). You will see one IKS cluster, which is deployed on-prem. On main screen you see general information about its status - numbers of control plane nodes, workers, associated profile. Using three dots button on most right column you open menu where you are able to download kubeconfig, undeploy Cluster or Open TAC case. Explore deployed IKS cluster Please, click at name of deployed IKS cluster. Now you are inside the selected cluster, where you find more details about deployment. Dashboard shows information about cluster condition, version of the K8s installed, CNI and more. By selecting Operate Tab, you find more information about profile and policy usage. Please explore rest of the Sections to see information under them. 4. Intersight Dashboard Configure Another section in Navigation pane is \"Configure\". Under \"Configure > Profiles\" you find definition of IKS profile used for CLUS-IKS-1 K8s deployment you just explored. IKS Profile wizzard When you open existing profile, you will be able to see progress wizzard. Tip In new deployment you need to create all Pools and Polices for IKS beforehand. In the wizzard below, you will see already selected polices. It is explained in next section. Warning PLEASE, DO NOT EDIT ANY OF THE POLICES UNDER THE PROFILE - IT MAY CAUSE DESTROY OF YOUR IKS CLUSTER. Explore it only and CLOSE in the END - DON'T DEPLOY CHANGES. First page of the wizzard is asking Name of the Cluster Second page contains information about cluster in general. You find there IP Pool, LoadBalancer IP counts, DNS/NTP policy, etc. Next one, its K8s Control Plane node definition. It is possible to configure desired state of CP nodes, minumum/maximum, K8s version, IP pools. It is first place where user must specify VM infra policy which contain vCenter information (basically definition where to deploy CP nodes). VM Machine policy contain information about VM size (RAM, CPU, disk size). Fourth page its definition of worker nodes. Similar to control-plane node definition, its possible to configure size of cluster, VM polices, IP pools, K8s version. Next section contain ADD-Ons definition. Cisco Intersight give you an option to install K8s add-ons automatically during IKS cluster deployments. One of the ADD-on installed is Kuberenetes Dashboard. Final page is summary of the entire profile, where is \"DEPLOY\" button. DO NOT DEPLOY PROFILE, BUT CLOSE IT. IKS Configure Polices Important section when working with Cisco Intersight is Polices . Here user needs to configure parameters of deployments before start doing profiles. In the LAB we use seven polices as shown on the figure below. Table gives general information about Names and Policy Types, time of creation/update. By clicking on the policy name, its possible to check details and edit it.","title":"Explore Cisco Intersight Dashboard"},{"location":"intersight.html#explore-cisco-intersight-dashboard","text":"","title":"Explore Cisco Intersight Dashboard"},{"location":"intersight.html#1-accessing-cisco-intersight-platform","text":"Cisco Intersight Platform manages Kuberenetes clusters in the private infrasturcture. You will have access to dedicated instance of Cisco Intersight, from which you will manage your own Kuberenetes Clusters used later on to deploy application. Please find login credentials and URL to your IKS instance below: URL: https://intersight.com/ User name: holcld2611+pod'X'@gmail.com Where X will be provided by proctor User password: CiscoLive!2022 Warning You can explore Cisco Intersight through the GUI, but please do not delete content already created.","title":"1. Accessing Cisco Intersight Platform"},{"location":"intersight.html#access-intersight-using-following-steps","text":"Open WebBrowser (Chrome) and connect to https://intersight.com Select Sign In with Cisco ID option on the first screen - as shown on figure below. Enter e-mail provided by proctor to your POD: Click next and fill password. Tip Credentials for the session can be found in WIL Assistant portal. In case of issues, ask proctor for help.","title":"Access Intersight using following steps:"},{"location":"intersight.html#2-intersigh-dashboard-target","text":"While you are logged to Cisco Intersight, first page you see is Monitor section. Navigate to Admin section in the bottom of Navigation Pane on Left. Click on Target Target section shows already Claimed services - in our case it is Cisco Intersight Assist installed \"on-prem\" together with vCenter. You can see status of claim, date and who executed it. Click on our Intersight Assist - iva.dcloud.cisco.com to see details of the Target.","title":"2. Intersigh Dashboard Target"},{"location":"intersight.html#3-intersight-dashboard-operate","text":"Most information from claimed targets are visible under Operate section. The section is divided to two sub categories: Virtualization and Kuberenetes.","title":"3. Intersight Dashboard Operate"},{"location":"intersight.html#virtualization","text":"When exloring first of them, you will see information about all VMs managed by vCenter claimed in Target section. By using three dots button on right column, you are able to manage each of VM: Option when open the Menu : On the other Tabs, you can verify details of Hosts, Storage, etc. Please explore yourself rest of the tab when interested.","title":"Virtualization"},{"location":"intersight.html#kuberenetes","text":"Second section contain informations about deployed Intersight Kuberenetes Clusters (IKS). You will see one IKS cluster, which is deployed on-prem. On main screen you see general information about its status - numbers of control plane nodes, workers, associated profile. Using three dots button on most right column you open menu where you are able to download kubeconfig, undeploy Cluster or Open TAC case.","title":"Kuberenetes"},{"location":"intersight.html#explore-deployed-iks-cluster","text":"Please, click at name of deployed IKS cluster. Now you are inside the selected cluster, where you find more details about deployment. Dashboard shows information about cluster condition, version of the K8s installed, CNI and more. By selecting Operate Tab, you find more information about profile and policy usage. Please explore rest of the Sections to see information under them.","title":"Explore deployed IKS cluster"},{"location":"intersight.html#4-intersight-dashboard-configure","text":"Another section in Navigation pane is \"Configure\". Under \"Configure > Profiles\" you find definition of IKS profile used for CLUS-IKS-1 K8s deployment you just explored.","title":"4. Intersight Dashboard Configure"},{"location":"intersight.html#iks-profile-wizzard","text":"When you open existing profile, you will be able to see progress wizzard. Tip In new deployment you need to create all Pools and Polices for IKS beforehand. In the wizzard below, you will see already selected polices. It is explained in next section. Warning PLEASE, DO NOT EDIT ANY OF THE POLICES UNDER THE PROFILE - IT MAY CAUSE DESTROY OF YOUR IKS CLUSTER. Explore it only and CLOSE in the END - DON'T DEPLOY CHANGES. First page of the wizzard is asking Name of the Cluster Second page contains information about cluster in general. You find there IP Pool, LoadBalancer IP counts, DNS/NTP policy, etc. Next one, its K8s Control Plane node definition. It is possible to configure desired state of CP nodes, minumum/maximum, K8s version, IP pools. It is first place where user must specify VM infra policy which contain vCenter information (basically definition where to deploy CP nodes). VM Machine policy contain information about VM size (RAM, CPU, disk size). Fourth page its definition of worker nodes. Similar to control-plane node definition, its possible to configure size of cluster, VM polices, IP pools, K8s version. Next section contain ADD-Ons definition. Cisco Intersight give you an option to install K8s add-ons automatically during IKS cluster deployments. One of the ADD-on installed is Kuberenetes Dashboard. Final page is summary of the entire profile, where is \"DEPLOY\" button. DO NOT DEPLOY PROFILE, BUT CLOSE IT.","title":"IKS Profile wizzard"},{"location":"intersight.html#iks-configure-polices","text":"Important section when working with Cisco Intersight is Polices . Here user needs to configure parameters of deployments before start doing profiles. In the LAB we use seven polices as shown on the figure below. Table gives general information about Names and Policy Types, time of creation/update. By clicking on the policy name, its possible to check details and edit it.","title":"IKS Configure Polices"},{"location":"lab-check.html","text":"","title":"Lab check"},{"location":"nd.html","text":"Infrastructure configuration - MultiSite configuration This lab section describes Nexus Dashboard functionalities. Nexus Dashboard (ND) Cisco Nexus Dashboard is a single launch point to monitor and scale across different sites, whether it is Cisco Application Centric Infrastructure\u2122 (Cisco ACI\u00ae) fabric controllers, the Cisco\u00ae Application Policy Infrastructure Controller (APIC), Cisco Nexus Dashboard Fabric Controller (NDFC), or a Cloud APIC running in a public cloud provider environment. Cisco NX-OS with Cisco Nexus Dashboard Fabric Controller (NDFC) \u2013 formerly Cisco Data Center Network Manager (Cisco DCNM) \u2013 is available as a service on the Cisco Nexus Dashboard. With third-party services integrated in Nexus Dashboard, NetOps can achieve command and control over global network fabrics, optimizing performance and attaining insights into data center and cloud operations. Using Cisco Nexus Dashboard, DevOps can improve the application deployment experience for multicloud applications Infrastructure-as-Code (IaC) integrations. Developers describe in code the networking components and resources needed to run an application in a data center or cloud. Nexus Dashboard overview Go to your RDP session previously opened. Open Chrome web browser and from bookmarks select Nexus Dashboard . Connect with login credentials: Username: admin User password: C1sco12345 After login to your Nexus Dashboard, you will land in \"One view\" section. It is usually a world map or table with your sites configured to be maintained from ND. You can zoom in/out to see it more in details. By changing the view to table by using slider marked in figure above, you will have such details on the screen. Launch link will open you APIC/CNC GUI in new web browser tab. By Clicking on Admin Console on Left navigation panel (marked red on figure above), you will be moved to System Overview Dashboard. Here you find information about your sites as well as Nexus Dashboard server status. You can check whether system is healthy, how many nodes in ND cluster you have, how many PODs/Deployments and Services are up and running. Simple graphical monitoring of entire system in one place. To explore more, you can click on Left Navigation panel to find out details information about Nexus Dashboard: Sites - sites configured to be maintain by ND cluster Services - Services installed on Nexus Dashboard - in our case Orchestrator (NDO) System Resources - details about ND cluster - PODs/Deployments/Services, etc Operations - Firmware management, backups, etc Infrastructure - ND infra Administrative - AAA configuration In next section you will explore Nexus Dashboard Orchestrator - service which is running on ND cluster.","title":"Dashboard overview"},{"location":"nd.html#infrastructure-configuration-multisite-configuration","text":"This lab section describes Nexus Dashboard functionalities.","title":"Infrastructure configuration - MultiSite configuration"},{"location":"nd.html#nexus-dashboard-nd","text":"Cisco Nexus Dashboard is a single launch point to monitor and scale across different sites, whether it is Cisco Application Centric Infrastructure\u2122 (Cisco ACI\u00ae) fabric controllers, the Cisco\u00ae Application Policy Infrastructure Controller (APIC), Cisco Nexus Dashboard Fabric Controller (NDFC), or a Cloud APIC running in a public cloud provider environment. Cisco NX-OS with Cisco Nexus Dashboard Fabric Controller (NDFC) \u2013 formerly Cisco Data Center Network Manager (Cisco DCNM) \u2013 is available as a service on the Cisco Nexus Dashboard. With third-party services integrated in Nexus Dashboard, NetOps can achieve command and control over global network fabrics, optimizing performance and attaining insights into data center and cloud operations. Using Cisco Nexus Dashboard, DevOps can improve the application deployment experience for multicloud applications Infrastructure-as-Code (IaC) integrations. Developers describe in code the networking components and resources needed to run an application in a data center or cloud.","title":"Nexus Dashboard (ND)"},{"location":"nd.html#nexus-dashboard-overview","text":"Go to your RDP session previously opened. Open Chrome web browser and from bookmarks select Nexus Dashboard . Connect with login credentials: Username: admin User password: C1sco12345 After login to your Nexus Dashboard, you will land in \"One view\" section. It is usually a world map or table with your sites configured to be maintained from ND. You can zoom in/out to see it more in details. By changing the view to table by using slider marked in figure above, you will have such details on the screen. Launch link will open you APIC/CNC GUI in new web browser tab. By Clicking on Admin Console on Left navigation panel (marked red on figure above), you will be moved to System Overview Dashboard. Here you find information about your sites as well as Nexus Dashboard server status. You can check whether system is healthy, how many nodes in ND cluster you have, how many PODs/Deployments and Services are up and running. Simple graphical monitoring of entire system in one place. To explore more, you can click on Left Navigation panel to find out details information about Nexus Dashboard: Sites - sites configured to be maintain by ND cluster Services - Services installed on Nexus Dashboard - in our case Orchestrator (NDO) System Resources - details about ND cluster - PODs/Deployments/Services, etc Operations - Firmware management, backups, etc Infrastructure - ND infra Administrative - AAA configuration In next section you will explore Nexus Dashboard Orchestrator - service which is running on ND cluster.","title":"Nexus Dashboard overview"},{"location":"ndo-tenant.html","text":"Nexus Dasboard Orchestrator Tenant configuration In this section you will configure your first Tenant, stretched between AWS and onprem. Following diagram shows what is to about to be configured. Tenant Creation on NDO Info Configuration will be fully deployed from Nexus Dashboard Orchestrator . All activities - except AWS Trust - will be done at NDO GUI. On the Left navigation page click \"Application Management\" -> \"Tenant\" and then \"Add Tenant\" Fill in Tenant details for name and description Display Name: Tenant-01 Descrption: CL23 AMS Tenant-01 Associate Tenant to both Sites by checking the checkbox next to it. Note For now you are not able to Save this configuration with red marking on AWS Site. Click the Pencil button at the end of the site line to complete configuration. Additional setting are needed for CNC, so it knows which Tenant ID to use on AWS. cAPIC site configuration For AWS site we have 2 options - Untrusted with Cloud Access key and Secret or Trusted . In our case we would be using Trusted configuration. Each Tenant created on NDO associated to AWS Cloud required sepearete Account ID on AWS site. You can find your AWS credetials in POD Details at WILASSISTANT Dasboard. Fill in with your POD AWS User-Account ID and select Access Type as Trusted , hit Save . Now configuration can be saved, leave assocaited user list empty as there are no additional users and hit Save . On the Tenant list you should see Tenant-01 created an assigned to 2 Sites. In next step we will make AWS trust configuration, so CNC can make changes to AWS objects. You can find details about this process under Resources in Appendixes section of this Lab Guide.","title":"NDO Tenant configuration"},{"location":"ndo-tenant.html#nexus-dasboard-orchestrator-tenant-configuration","text":"In this section you will configure your first Tenant, stretched between AWS and onprem. Following diagram shows what is to about to be configured.","title":"Nexus Dasboard Orchestrator Tenant configuration"},{"location":"ndo-tenant.html#tenant-creation-on-ndo","text":"Info Configuration will be fully deployed from Nexus Dashboard Orchestrator . All activities - except AWS Trust - will be done at NDO GUI. On the Left navigation page click \"Application Management\" -> \"Tenant\" and then \"Add Tenant\" Fill in Tenant details for name and description Display Name: Tenant-01 Descrption: CL23 AMS Tenant-01 Associate Tenant to both Sites by checking the checkbox next to it. Note For now you are not able to Save this configuration with red marking on AWS Site. Click the Pencil button at the end of the site line to complete configuration. Additional setting are needed for CNC, so it knows which Tenant ID to use on AWS. cAPIC site configuration For AWS site we have 2 options - Untrusted with Cloud Access key and Secret or Trusted . In our case we would be using Trusted configuration. Each Tenant created on NDO associated to AWS Cloud required sepearete Account ID on AWS site. You can find your AWS credetials in POD Details at WILASSISTANT Dasboard. Fill in with your POD AWS User-Account ID and select Access Type as Trusted , hit Save . Now configuration can be saved, leave assocaited user list empty as there are no additional users and hit Save . On the Tenant list you should see Tenant-01 created an assigned to 2 Sites. In next step we will make AWS trust configuration, so CNC can make changes to AWS objects. You can find details about this process under Resources in Appendixes section of this Lab Guide.","title":"Tenant Creation on NDO"},{"location":"ndo.html","text":"Nexus Dashboard Orchestrator(NDO) Cisco Nexus Dashboard Orchestrator (NDO) provides consistent network and policy orchestration, scalability, and disaster recovery across multiple data centers through a single pane of glass while allowing the data center to go wherever the data is. NDO allows you to interconnect separate Cisco\u00ae Application Centric Infrastructure (Cisco ACI\u00ae) sites, Cisco Cloud ACI sites, and Cisco Nexus Dashboard Fabric Controller (NDFC) sites, each managed by its own controller (APIC cluster, NDFC cluster, or Cloud Network Controller instances in a public cloud). The on-premises sites (ACI or NDFC in the future) can be extended to different public clouds for hybrid-cloud deployments while cloud-first installations can be extended to multi-cloud deployments without on-premises sites. The single-pane network interconnect policy management and the consistent network workload and segmentation policy provided by NDO allows monitoring the health of the interconnected fabrics, enforcement of segmentation and security policies, and performance of all tasks required to define tenant intersite policies in APIC, NDFC, and CNC sites. Key features and benefits Single pane of glass for administration and orchestration of multiple networking fabrics for both Cisco ACI and NDFC Automation of the configuration and management of intersite network interconnects across an IP backbone for both Cisco ACI and NDFC Consistent multitenant policy across multiple sites, which allows IP mobility, disaster recovery, and active/active use cases for data centers Capability to map tenants, applications, and associated networks to specific availability domains within the Cisco Multi-Site architecture for both Cisco ACI and NDFC Hybrid cloud and multi-cloud orchestration supporting on-premises Cisco ACI sites and public cloud sites (AWS and Azure) Capability to have multi-cloud ACI deployments without on-premises sites Scale out sites and leaf switches based on resource growth Use cases There are several uses of Cisco Nexus Dashboard Orchestrator. Some of the main use cases include: Large scale data center deployment Data center interconnectivity Cisco NDO multi-domain integrations Hybrid cloud and multi-cloud Service provider \u2013 5G telco Let's now explore possibilites of Cisco Nexus Dashboard Orchestrator! 1. NDO Service Introduction While you are connected to Nexus Dashboard GUI, now you will explore NDO dashboard. Follow the instructions. Nexus Dashboard Orchestrator is already installed and enabled under \"Service Catalog\" inside \"Installed Service\" Hit \"Open\" button in order to login You will be now redirected to Nexus Dashboard Orchestrator Dashboard. 2. NDO Overview NDO Dashboard is very similar to ND Dashboard. You will see world map with sites located on them. You can change view to Table and get such summary on the screen. Similar to ND, Left navigation panel is divided to sections. You can find there: Sites - sites enablement under NDO. Sites are configured in ND itself but to be Managed by NDO must be explicitly configured in NDO Sites menu. Application Management - under this section you find all components for Logical deployment of tenants. You will use this section while configuring Use-Cases in a moment. Fabric Management - configuration of Fabric and Access Polices to be apply at sites Operations - Backup/Restore/Updates etc Infrastructure - Site connectivity - definition of overlay-1 VXLAN evpn, OSPF, eBGP - described on next section Integration - Possibile integration with SDWAN or DNAC On next page you will look on config definition for Inter-site communication.","title":"Orchestrator overview"},{"location":"ndo.html#nexus-dashboard-orchestratorndo","text":"Cisco Nexus Dashboard Orchestrator (NDO) provides consistent network and policy orchestration, scalability, and disaster recovery across multiple data centers through a single pane of glass while allowing the data center to go wherever the data is. NDO allows you to interconnect separate Cisco\u00ae Application Centric Infrastructure (Cisco ACI\u00ae) sites, Cisco Cloud ACI sites, and Cisco Nexus Dashboard Fabric Controller (NDFC) sites, each managed by its own controller (APIC cluster, NDFC cluster, or Cloud Network Controller instances in a public cloud). The on-premises sites (ACI or NDFC in the future) can be extended to different public clouds for hybrid-cloud deployments while cloud-first installations can be extended to multi-cloud deployments without on-premises sites. The single-pane network interconnect policy management and the consistent network workload and segmentation policy provided by NDO allows monitoring the health of the interconnected fabrics, enforcement of segmentation and security policies, and performance of all tasks required to define tenant intersite policies in APIC, NDFC, and CNC sites.","title":"Nexus Dashboard Orchestrator(NDO)"},{"location":"ndo.html#key-features-and-benefits","text":"Single pane of glass for administration and orchestration of multiple networking fabrics for both Cisco ACI and NDFC Automation of the configuration and management of intersite network interconnects across an IP backbone for both Cisco ACI and NDFC Consistent multitenant policy across multiple sites, which allows IP mobility, disaster recovery, and active/active use cases for data centers Capability to map tenants, applications, and associated networks to specific availability domains within the Cisco Multi-Site architecture for both Cisco ACI and NDFC Hybrid cloud and multi-cloud orchestration supporting on-premises Cisco ACI sites and public cloud sites (AWS and Azure) Capability to have multi-cloud ACI deployments without on-premises sites Scale out sites and leaf switches based on resource growth","title":"Key features and benefits"},{"location":"ndo.html#use-cases","text":"There are several uses of Cisco Nexus Dashboard Orchestrator. Some of the main use cases include: Large scale data center deployment Data center interconnectivity Cisco NDO multi-domain integrations Hybrid cloud and multi-cloud Service provider \u2013 5G telco Let's now explore possibilites of Cisco Nexus Dashboard Orchestrator!","title":"Use cases"},{"location":"ndo.html#1-ndo-service-introduction","text":"While you are connected to Nexus Dashboard GUI, now you will explore NDO dashboard. Follow the instructions. Nexus Dashboard Orchestrator is already installed and enabled under \"Service Catalog\" inside \"Installed Service\" Hit \"Open\" button in order to login You will be now redirected to Nexus Dashboard Orchestrator Dashboard.","title":"1. NDO Service Introduction"},{"location":"ndo.html#2-ndo-overview","text":"NDO Dashboard is very similar to ND Dashboard. You will see world map with sites located on them. You can change view to Table and get such summary on the screen. Similar to ND, Left navigation panel is divided to sections. You can find there: Sites - sites enablement under NDO. Sites are configured in ND itself but to be Managed by NDO must be explicitly configured in NDO Sites menu. Application Management - under this section you find all components for Logical deployment of tenants. You will use this section while configuring Use-Cases in a moment. Fabric Management - configuration of Fabric and Access Polices to be apply at sites Operations - Backup/Restore/Updates etc Infrastructure - Site connectivity - definition of overlay-1 VXLAN evpn, OSPF, eBGP - described on next section Integration - Possibile integration with SDWAN or DNAC On next page you will look on config definition for Inter-site communication.","title":"2. NDO Overview"},{"location":"object-map.html","text":"ACI to Public Cloud object mapping Before you move to configuration spend a moment at Policy Object mapping between ACI onprem and AWS. Cloud Netowrk Controller is using cloud native objects to map ACI constructins into various of public clouds. ACI to AWS object mapping Below you can find how ACI policy model is mapped to AWS cloud native objects","title":"ACI to Public Cloud Object mapping"},{"location":"object-map.html#aci-to-public-cloud-object-mapping","text":"Before you move to configuration spend a moment at Policy Object mapping between ACI onprem and AWS. Cloud Netowrk Controller is using cloud native objects to map ACI constructins into various of public clouds.","title":"ACI to Public Cloud object mapping"},{"location":"object-map.html#aci-to-aws-object-mapping","text":"Below you can find how ACI policy model is mapped to AWS cloud native objects","title":"ACI to AWS object mapping"},{"location":"pod1.html","text":"POD1 LABDCN-2542 Lab access details AWS Resources Warning Make sure to use correct account as indicated in lab guide! AWS Infra resources Name Username Password Public IPv4 address Description ND1 admin tobeprovided tobeprovided Nexus Dashboard appliance Cisco Cloud Network Controller admin tobeprovided tobeprovided CNC-AWS-01 Cloud Network Controller AWS User Tenant AWS User Account ID: 28374823xxaassxx AWS User Account Username: student AWS User Account Password: tobeprovided","title":"POD1 LABDCN-2542 Lab access details"},{"location":"pod1.html#pod1-labdcn-2542-lab-access-details","text":"","title":"POD1 LABDCN-2542 Lab access details"},{"location":"pod1.html#aws-resources","text":"Warning Make sure to use correct account as indicated in lab guide!","title":"AWS Resources"},{"location":"pod1.html#aws-infra-resources","text":"Name Username Password Public IPv4 address Description ND1 admin tobeprovided tobeprovided Nexus Dashboard appliance Cisco Cloud Network Controller admin tobeprovided tobeprovided CNC-AWS-01 Cloud Network Controller","title":"AWS Infra resources"},{"location":"pod1.html#aws-user-tenant","text":"AWS User Account ID: 28374823xxaassxx AWS User Account Username: student AWS User Account Password: tobeprovided","title":"AWS User Tenant"},{"location":"resources.html","text":"Resources from the LAB In this section you can find necessary files used during the Lab to download. Cisco Cloud Network Controller Solution Cisco Cloud Network Controller Main Page Cisco Cloud Network Controller Solution Overview White Papers Cisco Cloud ACI on AWS White Paper Cisco Cloud ACI on Microsoft Azure White Paper Case Studies Cisco ACI Case Studies Installation Guides AWS Installation Guide Azure Installation Guide Google Cloud Installation Guide","title":"Resources"},{"location":"resources.html#resources-from-the-lab","text":"In this section you can find necessary files used during the Lab to download.","title":"Resources from the LAB"},{"location":"resources.html#cisco-cloud-network-controller-solution","text":"Cisco Cloud Network Controller Main Page Cisco Cloud Network Controller Solution Overview","title":"Cisco Cloud Network Controller Solution"},{"location":"resources.html#white-papers","text":"Cisco Cloud ACI on AWS White Paper Cisco Cloud ACI on Microsoft Azure White Paper","title":"White Papers"},{"location":"resources.html#case-studies","text":"Cisco ACI Case Studies","title":"Case Studies"},{"location":"resources.html#installation-guides","text":"AWS Installation Guide Azure Installation Guide Google Cloud Installation Guide","title":"Installation Guides"},{"location":"restAPI.html","text":"Cisco ACI rest API In this lab section you will create and execute list of API calls to configure ACI managed objects as well as read data from existing polices and devices. After this section you should feel comfortable with running simple automation tasks and build your own tasks for further customizations. You will be familiar with Postman dashboard and general idea of running request individually or as a collection defined. 1 Define restAPI calls under Collection Now is a time to work with our requests to GET or POST restAPI. You will define couple of them during the LAB. Figure below shows where to navigate to create new request in your collection. Every API command have predefined structure. Administrator needs to specify URI for correct object are you going to work with. Everything is described in details under the following link: Cisco ACI API Configuration guide . The figure below shows structure of every API call: 1.1 Create ACI login request First request when working with ACI must be authentication. Without having your session authenticated, no API calls can be successfully executed. Information about Authentication request structure can be found here: Cisco ACI Authentication API . Define it in Postman: In New Request section specify: 1) Name of Request ACI Login 2) Select Method to be POST 3) Enter request URL https://{{apic}}/api/aaaLogin.json Note Please notice that it's first place you use your Environment Variable = apic. In JSON definition every variable is closed by {{ }} . 4) Move to Body section for further work with your request Once you open Body section, you need to select type of data to be raw and coding to JSON . Now you can copy code defined below to body section of your request. aaaLogin { \"aaaUser\" : { \"attributes\" : { \"name\" : \"{{user}}\" , \"pwd\" : \"{{password}}\" } } } Yellow highlighted text in the json code above will use variables you defined in Environment at the beginning of the Lab. Now your Request should looks like on the figure below. It's time to TEST it! . Click on Send button. Make sure your environment ACI-dcloud is selected - above the Send button. When works as expected you will be authenticated to APIC. Responce from the APIC is visible in the section at your screen. Let's go together across results you see on the screen. First is a Status 200 OK - means APIC server accepted your request, execute it and responce with data. In the REPLY Body you see JSON structure data. You can see token which is your authentication token. RefreshTimeoutSeconds set to 600 seconds = 10 minutes. Token is valid for 10 minutes and after that period will expire. Please scroll down across the response body yourself. Look for such information: userName sessionId aaaWriteRoles Write them down to notepad. Register your ACI Fabric Switches in APIC Lab is clear and leaf/spines requires registration. You can automate it using Postman. Configure new Postman POST Request with code downloaded from here use URI: https://{{apic}}/api/node/class/fabricNode.json Contact Instructor in case of issues. 1.2 Get Information About a Node POST is not only one request type. You can configure your ACI fabric, but you can use restAPI to pull data you are interest to analyse. This excercise is about writing a GET Request under already created ACI collection. As an example you will pull information about Leaf node-101 system in json . Name your new Request Node-101-topsystem check Copy the URI to GET request: https://{{apic}}/api/mo/topology/pod-1/node-101/sys.json Save new Request and Click Send . Note It may happen that your API Token expired - remember its valid only 10 minutes. If you experience it, go to ACI Login Request and Send it again to re-authenticate. In the Body responce you can verify topsystem information about Leaf in your Fabric. You can pull data about AccessPolices, Interfaces as well as Logical construct - Tenant, EPGs, VRFs etc. The idea is always same - specify correct URL. Another test will be quering information about APIC node and current firmware version running on it. Please Create another Request under your Collection with following setup: Name of Request: Get Running Firmware GET request URI: https://{{apic}}/api/mo/topology/pod-1/node-1/sys/ctrlrfwstatuscont.json?query-target=subtree&target-subtree-class=firmwareCtrlrRunning Please observe that current query is composed with parameters. Before we pulled data directly from top-system of our ACI Leaf. Now you are narrowing the output to only sub-class you are intrested about. Try to delete part of URI ?query-target=subtree&target-subtree-class=firmwareCtrlrRunning and do Send again. 2 ACI Access Polices Every ACI Fabric configuration starts with Access Polices. You need to define ACI objects which represent single physical port configurations. To remind relationships between objects, please check following figure. Usually Interface Polices are configured once and re-used like LLDP, LACP, CDP, etc. However our LAB is empty and you need to start from zero. Following sections will help you to prepare your API requests. 2.1 Interface Policies You will configure LACP Policy, LLDP_ON, LINK-10G. Copy each of the Policy json definition to individual POST requests under your ACI Collection in Postman. Use URI to POST: https://{{apic}}/api/node/mo/uni.json LACP_ACTIVE { \"lacpLagPol\" : { \"attributes\" : { \"dn\" : \"uni/infra/lacplagp-LACP_ACTIVE\" , \"ctrl\" : \"fast-sel-hot-stdby,graceful-conv,susp-individual\" , \"name\" : \"LACP_ACTIVE\" , \"mode\" : \"active\" , \"rn\" : \"lacplagp-LACP_ACTIVE\" , \"status\" : \"created\" }, \"children\" : [] } } LLDP_ON { \"lldpIfPol\" : { \"attributes\" : { \"dn\" : \"uni/infra/lldpIfP-LLDP_ON\" , \"name\" : \"LLDP_ON\" , \"rn\" : \"lldpIfP-LLDP_ON\" , \"status\" : \"created\" }, \"children\" : [] } } LINK-10G { \"fabricHIfPol\" : { \"attributes\" : { \"dn\" : \"uni/infra/hintfpol-LINK-10G\" , \"name\" : \"LINK-10G\" , \"speed\" : \"10G\" , \"rn\" : \"hintfpol-LINK-10G\" , \"status\" : \"created\" }, \"children\" : [] } } 2.2 VLANs, Domains and AAEPs Create VLAN pool with vlan-range, associate it with Physical Domain and AAEP. Now you will change an approach. Your task is to create VLAN Pool with APIC GUI and capture json code from API Inspector . Connect to your APIC using bookmark in dCloud workstation Chrome. Navigate to Access Polices -> Pools -> VLAN. When you are there, Click on Help and Tools menu in APIC Dashboard and Click on Show API Inspector . In new window you will open API Inspector, where you can capture every API call send to the APIC server. Now you can start ADD VLAN-pool in APIC GUI. VLAN POOL NAME: vlan-pool Allocation Mode: Static Encap Blocks : 100-200, Inherit Allocation mode from parent, Role: External or On the wire encapsulations SUBMIT and open API Inspector in the another Chrome Window. Search for POST method. COPY now ALL POST URI from API Inspector to Postman request Body. You should capture data as shown in code below. h tt ps : //apic1.dcloud.cisco.com/api/node/mo/uni/infra/vlanns-[vlan-pool]-static.json payload { \"fvnsVlanInstP\" : { \"attributes\" : { \"dn\" : \"uni/infra/vlanns-[vlan-pool]-static\" , \"name\" : \"vlan-pool\" , \"allocMode\" : \"static\" , \"rn\" : \"vlanns-[vlan-pool]-static\" , \"status\" : \"created\" }, \"children\" : [ { \"fvnsEncapBlk\" : { \"attributes\" : { \"dn\" : \"uni/infra/vlanns-[vlan-pool]-static/from-[vlan-100]-to-[vlan-200]\" , \"from\" : \"vlan-100\" , \"to\" : \"vlan-200\" , \"rn\" : \"from-[vlan-100]-to-[vlan-200]\" , \"status\" : \"created\" }, \"children\" : [] } } ] } } Now it's time to build a request from data captured. You will use that request in the future for adding more vlan pools. First, copy URL and place it in Postman URL. Replace apic1.dcloud.cisco.com with our variable {{apic}}. Next replace vlanpool name in [ ] brakets. New URI looks like that: https://{{apic}}/api/node/mo/uni/infra/vlanns-[{{vlanpoolname}}]-static.json VLAN POOL { \"fvnsVlanInstP\" : { \"attributes\" : { \"dn\" : \"uni/infra/vlanns-[{{vlanpoolname}}]-static\" , \"name\" : \"{{vlanpoolname}}\" , \"allocMode\" : \"static\" , \"rn\" : \"vlanns-[{{vlanpoolname}}]-static\" , \"status\" : \"created\" }, \"children\" : [ { \"fvnsEncapBlk\" : { \"attributes\" : { \"dn\" : \"uni/infra/vlanns-[{{vlanpoolname}}]-static/from-[vlan-{{vlanstart}}]-to-[vlan-{{vlanend}}]\" , \"from\" : \"vlan-{{vlanstart}}\" , \"to\" : \"vlan-{{vlanend}}\" , \"rn\" : \"from-[vlan-{{vlanstart}}]-to-[vlan-{{vlanend}]\" , \"status\" : \"created\" }, \"children\" : [] } } ] } } Variables {{vlanpoolname}}, {{vlanstart}}, {{vlanend}} you can replace by hardcoded values in your code, or define values in Environment. Later today you will see the powerfull of variables using csv file for input data. Same approach please use in Domain and AAEP creation - Go to GUI, do Domain - PHY-DOM, associate with vlan-pool and AAEP - dcloud-AAEP. Observe API Inspector, capture the code and build those two requests in Postman. URI for AAEP Function: https://{{apic}}/api/node/mo/uni/infra.json AAEP { \"infraInfra\" : { \"attributes\" : { \"dn\" : \"uni/infra\" , \"status\" : \"modified\" }, \"children\" : [ { \"infraAttEntityP\" : { \"attributes\" : { \"dn\" : \"uni/infra/attentp-{{aaep}}\" , \"name\" : \"{{aaep}}\" , \"rn\" : \"attentp-{{aaep}}\" , \"status\" : \"created\" }, \"children\" : [] } }, { \"infraFuncP\" : { \"attributes\" : { \"dn\" : \"uni/infra/funcprof\" , \"status\" : \"modified\" }, \"children\" : [] } } ] } } Last one is Domain. Create Physical domain PHY-DOM with vlan pool associated vlan-pool and AAEP dcloud-AAEP . https://{{apic}}/api/node/mo/uni/phys-{{domain}}.json Domain { \"physDomP\" : { \"attributes\" : { \"dn\" : \"uni/phys-{{domain}}\" , \"name\" : \"{{domain}}\" , \"rn\" : \"phys-{{domain}}\" , \"status\" : \"created\" }, \"children\" : [ { \"infraRsVlanNs\" : { \"attributes\" : { \"tDn\" : \"uni/infra/vlanns-[{{vlanpoolname}}]-static\" , \"status\" : \"created\" }, \"children\" : [] } } ] } } Interesting fact about domain to AAEP association. Domain is sub-object to AAEP, you will not see AAEP under Domain definition, even you specify it in APIC GUI. In API you need to add it under AAEP. https://{{apic}}/api/node/mo/uni/infra.json Domain to AAEP association { \"infraAttEntityP\" : { \"attributes\" : { \"annotation\" : \"\" , \"descr\" : \"\" , \"dn\" : \"uni/infra/attentp-{{aaep}}\" , \"name\" : \"{{aaep}}\" , \"nameAlias\" : \"\" , \"ownerKey\" : \"\" , \"ownerTag\" : \"\" , \"userdom\" : \":all:\" }, \"children\" : [ { \"infraRsDomP\" : { \"attributes\" : { \"annotation\" : \"\" , \"tDn\" : \"uni/phys-{{domain}}\" , \"userdom\" : \":all:\" } } } ] } } 2.3 Interface Policy Group Lets now create one of the Interface policy group and use all created policy. Interface Policy Group VPC { \"infraAccBndlGrp\" : { \"attributes\" : { \"dn\" : \"uni/infra/funcprof/accbundle-intpolgrp-vpc-server1\" , \"lagT\" : \"node\" , \"name\" : \"intpolgrp-vpc-server1\" , \"rn\" : \"accbundle-intpolgrp-vpc-server1\" , \"status\" : \"created\" }, \"children\" : [ { \"infraRsAttEntP\" : { \"attributes\" : { \"tDn\" : \"uni/infra/attentp-dcloud-AAEP\" , \"status\" : \"created,modified\" }, \"children\" : [] } }, { \"infraRsHIfPol\" : { \"attributes\" : { \"tnFabricHIfPolName\" : \"LINK-10G\" , \"status\" : \"created,modified\" }, \"children\" : [] } }, { \"infraRsLldpIfPol\" : { \"attributes\" : { \"tnLldpIfPolName\" : \"LLDP_ON\" , \"status\" : \"created,modified\" }, \"children\" : [] } }, { \"infraRsLacpPol\" : { \"attributes\" : { \"tnLacpLagPolName\" : \"LACP_ACTIVE\" , \"status\" : \"created,modified\" }, \"children\" : [] } } ] } } Replace intpolgrp-vpc-server1 with intpolgrp-vpc-server2 to observe how simple you can add new VPC policy group using Postman. Warning Download three files to your dcloud workstation and using post in Postman or in APIC GUI upload it to access-polices. Without them, your VPC won't instanciate and cannot be used in later stage of the lab. JSON VPC Policy . JSON interface leaf profile . JSON switch leaf profile . Ask instructor for assistance if not clear. 3 ACI Tenant Upon now you created ACI AccessPolicies for your Tenant. Having JSON code for every object you will be able to unified variables across Postman Collection and then run all together using Variables from Environment or much better from CSV file. Now is time to create your tenant using JSON. It will be simple Tenant definition with one VRF and two bridge domains associated with two EPGs. Every EPG will be associated with created domain and statically binded to both VPC created, with VLAN from VLAN pool done by you perviously. Figure below shows our Logical Tenant. You need to define Tenant, VRF, Bridge-Domains, Application-Profile, EPGs with Domain association and static binding under EPG. Quite many of Requests to do in Postman. You can define all of them under one Request. Please, download a JSON file from JSON Tenant Ready . Once you have it on dcloud workstation, please create new Request under ACI Collection: NAME: CREATE TENANT-1 Method: POST URL: https://{{apic}}/api/node/mo/uni.json COPY now content from downloaded file to Body of the new request, Save and Send it. Note Please check your authentication token if still valid. Tenant-1 will be created with all structure shown in figure above. 3.1 Tenant components This section contain JSON codes necessary to create Tenant objects. 3.1.1 Tenant and VRF Code below can be used to create new Tenant and VRF. https://{{apic}}/api/node/mo/uni.json Tenant and VRF from CSV { \"fvTenant\" : { \"attributes\" : { \"annotation\" : \"\" , \"descr\" : \"\" , \"dn\" : \"uni/tn-{{tenantname}}\" , \"name\" : \"{{tenantname}}\" }, \"children\" : [ { \"fvCtx\" : { \"attributes\" : { \"name\" : \"{{vrfname}}\" }, \"children\" : [] } } ] } } 3.1.2 BD in existing tenant Code below can be used to create new Bridge-Domain in existing Tenant. https://{{apic}}/api/node/mo/uni.json Bridge-domain L3 from CSV { \"fvBD\" : { \"attributes\" : { \"name\" : \"{{bdname}}\" , \"dn\" : \"uni/tn-{{tenantname}}/BD-{{bdname}}\" , \"rn\" : \"BD-{{bdname}}\" , \"type\" : \"regular\" }, \"children\" : [ { \"fvSubnet\" : { \"attributes\" : { \"ip\" : \"{{ipaddress}}\" , \"ipDPLearning\" : \"enabled\" , \"scope\" : \"private\" } } }, { \"fvRsCtx\" : { \"attributes\" : { \"tnFvCtxName\" : \"{{vrfname}}\" } } } ] } } In case you need to add BD without IP address and unicast routing disabled, use code below. https://{{apic}}/api/node/mo/uni.json Bridge-domain L2 from CSV { \"fvTenant\" : { \"attributes\" : { \"dn\" : \"uni/tn-{{tenantname}}\" , \"status\" : \"modified\" }, \"children\" : [ { \"fvBD\" : { \"attributes\" : { \"dn\" : \"uni/tn-{{tenantname}}/BD-{{bdname}}\" , \"mac\" : \"00:22:BD:F8:19:FF\" , \"name\" : \"{{bdname}}\" , \"rn\" : \"BD-{{bdname}}\" , \"arpFlood\" : \"yes\" , \"ipLearning\" : \"no\" , \"unicastRoute\" : \"no\" , \"unkMacUcastAct\" : \"flood\" , \"type\" : \"regular\" , \"unkMcastAct\" : \"flood\" , \"status\" : \"created\" }, \"children\" : [ { \"fvRsCtx\" : { \"attributes\" : { \"tnFvCtxName\" : \"{{vrfname}}\" , \"status\" : \"created,modified\" }, \"children\" : [] } } ] } } ] } } Highlighted rows are specific for L2 Bridge-Domain. 3.1.3 Application Profile Component which contain all EPGs in the Tenant. https://{{apic}}/api/node/mo/uni.json Application Profile from CSV { \"fvAp\" : { \"attributes\" : { \"dn\" : \"uni/tn-{{tenantname}}/ap-{{ap}}\" , \"name\" : \"{{ap}}\" , \"rn\" : \"ap-{{ap}}\" , \"status\" : \"created\" } } } 3.1.4 EPGs in existing tenant/appprofiles and associated with domain https://{{apic}}/api/node/mo/uni.json Create EPG under existing application profile from CSV { \"fvAEPg\" : { \"attributes\" : { \"dn\" : \"uni/tn-{{tenantname}}/ap-{{ap}}/epg-{{epgname}}\" , \"name\" : \"{{epgname}}\" , \"rn\" : \"epg-{{epgname}}\" , \"status\" : \"created\" }, \"children\" : [ { \"fvRsBd\" : { \"attributes\" : { \"tnFvBDname\" : \"{{bdname}}\" , \"status\" : \"created,modified\" }, \"children\" : [] } }, { \"fvRsDomAtt\" : { \"attributes\" : { \"tDn\" : \"uni/phys-{{domain}}\" , \"status\" : \"created\" }, \"children\" : [] } } ] } } 4 Use CSV file for input data You are about to re-use work you did during all excercises. Untill now you run every request for static data, only once. Adding 100 Bridge-Domains, by changing \"Variables\" within the code would be problematic. Postman is giving an option to use external variable file to execute created requests in the collection. By running five requests from your Collection and using CSV file from location FILE CSV TO DOWNLOAD you will add new Tenant with one VRF, Applicatioin Profile, 22 Bridge-Domains and 22 EPGs associated to Physical Domain created before. All of this will be done in one Postman collection Run. 4.1 Run Collection requests Follow the instruction from the figure below: 1) Click on dots icon at your ACI dCloud collection folder 2) Select Run Collection You will be moved to new dashboard, where you will select Requests and input file. Deselect All, and then Select only needed requests: When you select Requests, go to Data \"Select File\" and choice downloaded CSV file - tenant-create.csv . Confirm that your file is selected as on the figure above. You can confirm amount of iterations in the top field Iterations . Note Do not forget to authenticate your Postman session using ACI Login request. I recommend to do it out of collection Run -- beforehand. Click Run ACI dCloud Blue button. 4.2 Verification after runing the collection When you successfully execute your collection, in Summary at the screen you will see results of every Iteration. When Request was accepted by APIC, result will be 200 OK . Every Iteration contain all five seleceted requests. You probably noticed that one of them is getting 400 Bad Request - \"Bridge Domain L3 from CSV\". Reason for that, is missing children attribute fvSubnet for all L2 bridge-domain in our CSV file. Now connect to APIC GUI and verify if tenant exists, check dashboard status. You successfully Created New Tenant with 22 EPGs and 22 BDs. Please, edit CSV file in notepad++ editor at your dCloud workstation, replace Tenantname with new and run your Postman Collection once again. Do you understand now power of simple automation?","title":"Cisco ACI rest API"},{"location":"restAPI.html#cisco-aci-rest-api","text":"In this lab section you will create and execute list of API calls to configure ACI managed objects as well as read data from existing polices and devices. After this section you should feel comfortable with running simple automation tasks and build your own tasks for further customizations. You will be familiar with Postman dashboard and general idea of running request individually or as a collection defined.","title":"Cisco ACI rest API"},{"location":"restAPI.html#1-define-restapi-calls-under-collection","text":"Now is a time to work with our requests to GET or POST restAPI. You will define couple of them during the LAB. Figure below shows where to navigate to create new request in your collection. Every API command have predefined structure. Administrator needs to specify URI for correct object are you going to work with. Everything is described in details under the following link: Cisco ACI API Configuration guide . The figure below shows structure of every API call:","title":"1 Define restAPI calls under Collection"},{"location":"restAPI.html#11-create-aci-login-request","text":"First request when working with ACI must be authentication. Without having your session authenticated, no API calls can be successfully executed. Information about Authentication request structure can be found here: Cisco ACI Authentication API . Define it in Postman: In New Request section specify: 1) Name of Request ACI Login 2) Select Method to be POST 3) Enter request URL https://{{apic}}/api/aaaLogin.json Note Please notice that it's first place you use your Environment Variable = apic. In JSON definition every variable is closed by {{ }} . 4) Move to Body section for further work with your request Once you open Body section, you need to select type of data to be raw and coding to JSON . Now you can copy code defined below to body section of your request. aaaLogin { \"aaaUser\" : { \"attributes\" : { \"name\" : \"{{user}}\" , \"pwd\" : \"{{password}}\" } } } Yellow highlighted text in the json code above will use variables you defined in Environment at the beginning of the Lab. Now your Request should looks like on the figure below. It's time to TEST it! . Click on Send button. Make sure your environment ACI-dcloud is selected - above the Send button. When works as expected you will be authenticated to APIC. Responce from the APIC is visible in the section at your screen. Let's go together across results you see on the screen. First is a Status 200 OK - means APIC server accepted your request, execute it and responce with data. In the REPLY Body you see JSON structure data. You can see token which is your authentication token. RefreshTimeoutSeconds set to 600 seconds = 10 minutes. Token is valid for 10 minutes and after that period will expire. Please scroll down across the response body yourself. Look for such information: userName sessionId aaaWriteRoles Write them down to notepad.","title":"1.1 Create ACI login request"},{"location":"restAPI.html#register-your-aci-fabric-switches-in-apic","text":"Lab is clear and leaf/spines requires registration. You can automate it using Postman. Configure new Postman POST Request with code downloaded from here use URI: https://{{apic}}/api/node/class/fabricNode.json Contact Instructor in case of issues.","title":"Register your ACI Fabric Switches in APIC"},{"location":"restAPI.html#12-get-information-about-a-node","text":"POST is not only one request type. You can configure your ACI fabric, but you can use restAPI to pull data you are interest to analyse. This excercise is about writing a GET Request under already created ACI collection. As an example you will pull information about Leaf node-101 system in json . Name your new Request Node-101-topsystem check Copy the URI to GET request: https://{{apic}}/api/mo/topology/pod-1/node-101/sys.json Save new Request and Click Send . Note It may happen that your API Token expired - remember its valid only 10 minutes. If you experience it, go to ACI Login Request and Send it again to re-authenticate. In the Body responce you can verify topsystem information about Leaf in your Fabric. You can pull data about AccessPolices, Interfaces as well as Logical construct - Tenant, EPGs, VRFs etc. The idea is always same - specify correct URL. Another test will be quering information about APIC node and current firmware version running on it. Please Create another Request under your Collection with following setup: Name of Request: Get Running Firmware GET request URI: https://{{apic}}/api/mo/topology/pod-1/node-1/sys/ctrlrfwstatuscont.json?query-target=subtree&target-subtree-class=firmwareCtrlrRunning Please observe that current query is composed with parameters. Before we pulled data directly from top-system of our ACI Leaf. Now you are narrowing the output to only sub-class you are intrested about. Try to delete part of URI ?query-target=subtree&target-subtree-class=firmwareCtrlrRunning and do Send again.","title":"1.2 Get Information About a Node"},{"location":"restAPI.html#2-aci-access-polices","text":"Every ACI Fabric configuration starts with Access Polices. You need to define ACI objects which represent single physical port configurations. To remind relationships between objects, please check following figure. Usually Interface Polices are configured once and re-used like LLDP, LACP, CDP, etc. However our LAB is empty and you need to start from zero. Following sections will help you to prepare your API requests.","title":"2  ACI Access Polices"},{"location":"restAPI.html#21-interface-policies","text":"You will configure LACP Policy, LLDP_ON, LINK-10G. Copy each of the Policy json definition to individual POST requests under your ACI Collection in Postman. Use URI to POST: https://{{apic}}/api/node/mo/uni.json LACP_ACTIVE { \"lacpLagPol\" : { \"attributes\" : { \"dn\" : \"uni/infra/lacplagp-LACP_ACTIVE\" , \"ctrl\" : \"fast-sel-hot-stdby,graceful-conv,susp-individual\" , \"name\" : \"LACP_ACTIVE\" , \"mode\" : \"active\" , \"rn\" : \"lacplagp-LACP_ACTIVE\" , \"status\" : \"created\" }, \"children\" : [] } } LLDP_ON { \"lldpIfPol\" : { \"attributes\" : { \"dn\" : \"uni/infra/lldpIfP-LLDP_ON\" , \"name\" : \"LLDP_ON\" , \"rn\" : \"lldpIfP-LLDP_ON\" , \"status\" : \"created\" }, \"children\" : [] } } LINK-10G { \"fabricHIfPol\" : { \"attributes\" : { \"dn\" : \"uni/infra/hintfpol-LINK-10G\" , \"name\" : \"LINK-10G\" , \"speed\" : \"10G\" , \"rn\" : \"hintfpol-LINK-10G\" , \"status\" : \"created\" }, \"children\" : [] } }","title":"2.1 Interface Policies"},{"location":"restAPI.html#22-vlans-domains-and-aaeps","text":"Create VLAN pool with vlan-range, associate it with Physical Domain and AAEP. Now you will change an approach. Your task is to create VLAN Pool with APIC GUI and capture json code from API Inspector . Connect to your APIC using bookmark in dCloud workstation Chrome. Navigate to Access Polices -> Pools -> VLAN. When you are there, Click on Help and Tools menu in APIC Dashboard and Click on Show API Inspector . In new window you will open API Inspector, where you can capture every API call send to the APIC server. Now you can start ADD VLAN-pool in APIC GUI. VLAN POOL NAME: vlan-pool Allocation Mode: Static Encap Blocks : 100-200, Inherit Allocation mode from parent, Role: External or On the wire encapsulations SUBMIT and open API Inspector in the another Chrome Window. Search for POST method. COPY now ALL POST URI from API Inspector to Postman request Body. You should capture data as shown in code below. h tt ps : //apic1.dcloud.cisco.com/api/node/mo/uni/infra/vlanns-[vlan-pool]-static.json payload { \"fvnsVlanInstP\" : { \"attributes\" : { \"dn\" : \"uni/infra/vlanns-[vlan-pool]-static\" , \"name\" : \"vlan-pool\" , \"allocMode\" : \"static\" , \"rn\" : \"vlanns-[vlan-pool]-static\" , \"status\" : \"created\" }, \"children\" : [ { \"fvnsEncapBlk\" : { \"attributes\" : { \"dn\" : \"uni/infra/vlanns-[vlan-pool]-static/from-[vlan-100]-to-[vlan-200]\" , \"from\" : \"vlan-100\" , \"to\" : \"vlan-200\" , \"rn\" : \"from-[vlan-100]-to-[vlan-200]\" , \"status\" : \"created\" }, \"children\" : [] } } ] } } Now it's time to build a request from data captured. You will use that request in the future for adding more vlan pools. First, copy URL and place it in Postman URL. Replace apic1.dcloud.cisco.com with our variable {{apic}}. Next replace vlanpool name in [ ] brakets. New URI looks like that: https://{{apic}}/api/node/mo/uni/infra/vlanns-[{{vlanpoolname}}]-static.json VLAN POOL { \"fvnsVlanInstP\" : { \"attributes\" : { \"dn\" : \"uni/infra/vlanns-[{{vlanpoolname}}]-static\" , \"name\" : \"{{vlanpoolname}}\" , \"allocMode\" : \"static\" , \"rn\" : \"vlanns-[{{vlanpoolname}}]-static\" , \"status\" : \"created\" }, \"children\" : [ { \"fvnsEncapBlk\" : { \"attributes\" : { \"dn\" : \"uni/infra/vlanns-[{{vlanpoolname}}]-static/from-[vlan-{{vlanstart}}]-to-[vlan-{{vlanend}}]\" , \"from\" : \"vlan-{{vlanstart}}\" , \"to\" : \"vlan-{{vlanend}}\" , \"rn\" : \"from-[vlan-{{vlanstart}}]-to-[vlan-{{vlanend}]\" , \"status\" : \"created\" }, \"children\" : [] } } ] } } Variables {{vlanpoolname}}, {{vlanstart}}, {{vlanend}} you can replace by hardcoded values in your code, or define values in Environment. Later today you will see the powerfull of variables using csv file for input data. Same approach please use in Domain and AAEP creation - Go to GUI, do Domain - PHY-DOM, associate with vlan-pool and AAEP - dcloud-AAEP. Observe API Inspector, capture the code and build those two requests in Postman. URI for AAEP Function: https://{{apic}}/api/node/mo/uni/infra.json AAEP { \"infraInfra\" : { \"attributes\" : { \"dn\" : \"uni/infra\" , \"status\" : \"modified\" }, \"children\" : [ { \"infraAttEntityP\" : { \"attributes\" : { \"dn\" : \"uni/infra/attentp-{{aaep}}\" , \"name\" : \"{{aaep}}\" , \"rn\" : \"attentp-{{aaep}}\" , \"status\" : \"created\" }, \"children\" : [] } }, { \"infraFuncP\" : { \"attributes\" : { \"dn\" : \"uni/infra/funcprof\" , \"status\" : \"modified\" }, \"children\" : [] } } ] } } Last one is Domain. Create Physical domain PHY-DOM with vlan pool associated vlan-pool and AAEP dcloud-AAEP . https://{{apic}}/api/node/mo/uni/phys-{{domain}}.json Domain { \"physDomP\" : { \"attributes\" : { \"dn\" : \"uni/phys-{{domain}}\" , \"name\" : \"{{domain}}\" , \"rn\" : \"phys-{{domain}}\" , \"status\" : \"created\" }, \"children\" : [ { \"infraRsVlanNs\" : { \"attributes\" : { \"tDn\" : \"uni/infra/vlanns-[{{vlanpoolname}}]-static\" , \"status\" : \"created\" }, \"children\" : [] } } ] } } Interesting fact about domain to AAEP association. Domain is sub-object to AAEP, you will not see AAEP under Domain definition, even you specify it in APIC GUI. In API you need to add it under AAEP. https://{{apic}}/api/node/mo/uni/infra.json Domain to AAEP association { \"infraAttEntityP\" : { \"attributes\" : { \"annotation\" : \"\" , \"descr\" : \"\" , \"dn\" : \"uni/infra/attentp-{{aaep}}\" , \"name\" : \"{{aaep}}\" , \"nameAlias\" : \"\" , \"ownerKey\" : \"\" , \"ownerTag\" : \"\" , \"userdom\" : \":all:\" }, \"children\" : [ { \"infraRsDomP\" : { \"attributes\" : { \"annotation\" : \"\" , \"tDn\" : \"uni/phys-{{domain}}\" , \"userdom\" : \":all:\" } } } ] } }","title":"2.2 VLANs, Domains and AAEPs"},{"location":"restAPI.html#23-interface-policy-group","text":"Lets now create one of the Interface policy group and use all created policy. Interface Policy Group VPC { \"infraAccBndlGrp\" : { \"attributes\" : { \"dn\" : \"uni/infra/funcprof/accbundle-intpolgrp-vpc-server1\" , \"lagT\" : \"node\" , \"name\" : \"intpolgrp-vpc-server1\" , \"rn\" : \"accbundle-intpolgrp-vpc-server1\" , \"status\" : \"created\" }, \"children\" : [ { \"infraRsAttEntP\" : { \"attributes\" : { \"tDn\" : \"uni/infra/attentp-dcloud-AAEP\" , \"status\" : \"created,modified\" }, \"children\" : [] } }, { \"infraRsHIfPol\" : { \"attributes\" : { \"tnFabricHIfPolName\" : \"LINK-10G\" , \"status\" : \"created,modified\" }, \"children\" : [] } }, { \"infraRsLldpIfPol\" : { \"attributes\" : { \"tnLldpIfPolName\" : \"LLDP_ON\" , \"status\" : \"created,modified\" }, \"children\" : [] } }, { \"infraRsLacpPol\" : { \"attributes\" : { \"tnLacpLagPolName\" : \"LACP_ACTIVE\" , \"status\" : \"created,modified\" }, \"children\" : [] } } ] } } Replace intpolgrp-vpc-server1 with intpolgrp-vpc-server2 to observe how simple you can add new VPC policy group using Postman. Warning Download three files to your dcloud workstation and using post in Postman or in APIC GUI upload it to access-polices. Without them, your VPC won't instanciate and cannot be used in later stage of the lab. JSON VPC Policy . JSON interface leaf profile . JSON switch leaf profile . Ask instructor for assistance if not clear.","title":"2.3 Interface Policy Group"},{"location":"restAPI.html#3-aci-tenant","text":"Upon now you created ACI AccessPolicies for your Tenant. Having JSON code for every object you will be able to unified variables across Postman Collection and then run all together using Variables from Environment or much better from CSV file. Now is time to create your tenant using JSON. It will be simple Tenant definition with one VRF and two bridge domains associated with two EPGs. Every EPG will be associated with created domain and statically binded to both VPC created, with VLAN from VLAN pool done by you perviously. Figure below shows our Logical Tenant. You need to define Tenant, VRF, Bridge-Domains, Application-Profile, EPGs with Domain association and static binding under EPG. Quite many of Requests to do in Postman. You can define all of them under one Request. Please, download a JSON file from JSON Tenant Ready . Once you have it on dcloud workstation, please create new Request under ACI Collection: NAME: CREATE TENANT-1 Method: POST URL: https://{{apic}}/api/node/mo/uni.json COPY now content from downloaded file to Body of the new request, Save and Send it. Note Please check your authentication token if still valid. Tenant-1 will be created with all structure shown in figure above.","title":"3  ACI Tenant"},{"location":"restAPI.html#31-tenant-components","text":"This section contain JSON codes necessary to create Tenant objects.","title":"3.1 Tenant components"},{"location":"restAPI.html#311-tenant-and-vrf","text":"Code below can be used to create new Tenant and VRF. https://{{apic}}/api/node/mo/uni.json Tenant and VRF from CSV { \"fvTenant\" : { \"attributes\" : { \"annotation\" : \"\" , \"descr\" : \"\" , \"dn\" : \"uni/tn-{{tenantname}}\" , \"name\" : \"{{tenantname}}\" }, \"children\" : [ { \"fvCtx\" : { \"attributes\" : { \"name\" : \"{{vrfname}}\" }, \"children\" : [] } } ] } }","title":"3.1.1 Tenant and VRF"},{"location":"restAPI.html#312-bd-in-existing-tenant","text":"Code below can be used to create new Bridge-Domain in existing Tenant. https://{{apic}}/api/node/mo/uni.json Bridge-domain L3 from CSV { \"fvBD\" : { \"attributes\" : { \"name\" : \"{{bdname}}\" , \"dn\" : \"uni/tn-{{tenantname}}/BD-{{bdname}}\" , \"rn\" : \"BD-{{bdname}}\" , \"type\" : \"regular\" }, \"children\" : [ { \"fvSubnet\" : { \"attributes\" : { \"ip\" : \"{{ipaddress}}\" , \"ipDPLearning\" : \"enabled\" , \"scope\" : \"private\" } } }, { \"fvRsCtx\" : { \"attributes\" : { \"tnFvCtxName\" : \"{{vrfname}}\" } } } ] } } In case you need to add BD without IP address and unicast routing disabled, use code below. https://{{apic}}/api/node/mo/uni.json Bridge-domain L2 from CSV { \"fvTenant\" : { \"attributes\" : { \"dn\" : \"uni/tn-{{tenantname}}\" , \"status\" : \"modified\" }, \"children\" : [ { \"fvBD\" : { \"attributes\" : { \"dn\" : \"uni/tn-{{tenantname}}/BD-{{bdname}}\" , \"mac\" : \"00:22:BD:F8:19:FF\" , \"name\" : \"{{bdname}}\" , \"rn\" : \"BD-{{bdname}}\" , \"arpFlood\" : \"yes\" , \"ipLearning\" : \"no\" , \"unicastRoute\" : \"no\" , \"unkMacUcastAct\" : \"flood\" , \"type\" : \"regular\" , \"unkMcastAct\" : \"flood\" , \"status\" : \"created\" }, \"children\" : [ { \"fvRsCtx\" : { \"attributes\" : { \"tnFvCtxName\" : \"{{vrfname}}\" , \"status\" : \"created,modified\" }, \"children\" : [] } } ] } } ] } } Highlighted rows are specific for L2 Bridge-Domain.","title":"3.1.2 BD in existing tenant"},{"location":"restAPI.html#313-application-profile","text":"Component which contain all EPGs in the Tenant. https://{{apic}}/api/node/mo/uni.json Application Profile from CSV { \"fvAp\" : { \"attributes\" : { \"dn\" : \"uni/tn-{{tenantname}}/ap-{{ap}}\" , \"name\" : \"{{ap}}\" , \"rn\" : \"ap-{{ap}}\" , \"status\" : \"created\" } } }","title":"3.1.3 Application Profile"},{"location":"restAPI.html#314-epgs-in-existing-tenantappprofiles-and-associated-with-domain","text":"https://{{apic}}/api/node/mo/uni.json Create EPG under existing application profile from CSV { \"fvAEPg\" : { \"attributes\" : { \"dn\" : \"uni/tn-{{tenantname}}/ap-{{ap}}/epg-{{epgname}}\" , \"name\" : \"{{epgname}}\" , \"rn\" : \"epg-{{epgname}}\" , \"status\" : \"created\" }, \"children\" : [ { \"fvRsBd\" : { \"attributes\" : { \"tnFvBDname\" : \"{{bdname}}\" , \"status\" : \"created,modified\" }, \"children\" : [] } }, { \"fvRsDomAtt\" : { \"attributes\" : { \"tDn\" : \"uni/phys-{{domain}}\" , \"status\" : \"created\" }, \"children\" : [] } } ] } }","title":"3.1.4 EPGs in existing tenant/appprofiles and associated with domain"},{"location":"restAPI.html#4-use-csv-file-for-input-data","text":"You are about to re-use work you did during all excercises. Untill now you run every request for static data, only once. Adding 100 Bridge-Domains, by changing \"Variables\" within the code would be problematic. Postman is giving an option to use external variable file to execute created requests in the collection. By running five requests from your Collection and using CSV file from location FILE CSV TO DOWNLOAD you will add new Tenant with one VRF, Applicatioin Profile, 22 Bridge-Domains and 22 EPGs associated to Physical Domain created before. All of this will be done in one Postman collection Run.","title":"4 Use CSV file for input data"},{"location":"restAPI.html#41-run-collection-requests","text":"Follow the instruction from the figure below: 1) Click on dots icon at your ACI dCloud collection folder 2) Select Run Collection You will be moved to new dashboard, where you will select Requests and input file. Deselect All, and then Select only needed requests: When you select Requests, go to Data \"Select File\" and choice downloaded CSV file - tenant-create.csv . Confirm that your file is selected as on the figure above. You can confirm amount of iterations in the top field Iterations . Note Do not forget to authenticate your Postman session using ACI Login request. I recommend to do it out of collection Run -- beforehand. Click Run ACI dCloud Blue button.","title":"4.1 Run Collection requests"},{"location":"restAPI.html#42-verification-after-runing-the-collection","text":"When you successfully execute your collection, in Summary at the screen you will see results of every Iteration. When Request was accepted by APIC, result will be 200 OK . Every Iteration contain all five seleceted requests. You probably noticed that one of them is getting 400 Bad Request - \"Bridge Domain L3 from CSV\". Reason for that, is missing children attribute fvSubnet for all L2 bridge-domain in our CSV file. Now connect to APIC GUI and verify if tenant exists, check dashboard status. You successfully Created New Tenant with 22 EPGs and 22 BDs. Please, edit CSV file in notepad++ editor at your dCloud workstation, replace Tenantname with new and run your Postman Collection once again. Do you understand now power of simple automation?","title":"4.2 Verification after runing the collection"},{"location":"use-case1.html","text":"Use-case 01 - Stretched VRF with EPG to EPG communication First use-case task will focus on: Configuration of stretched VRF inside Tenant-01 VPC CIDR configuration in AWS EPG configuration in both sides EC2 creation and EPG assigment Contract configuration to allow traffic flow Schema, Template configuration All logical polices and configuration are done in Nexus Dashboard Orchestratod via Schemas and Templates. Each Schema can have multiple Templates, but Template can belong to only one Schema. Each Template is also assocaited to one and only one Tenant. Template to site association will also define to which fabric (on prem or cloud) configration will be pushed, therefore is the smallest logical unit you can decide where changes are deployed. 1. Schema creation Navigate to Dashboard -> Application Management -> Schemas , then hit \"Add Schema\" Add Schema name and Description and hit \"Add\" button Name: Schema-T01 Description: Schema for Tenant-01 2. Template creation Under the Schema-T01, let's create first Template with \"Add New Template\" button For a Template type select \"ACI Multi-Cloud\" and hit \"Add\" On the right side of the template screen, we can customize the Tempalte Display Name, and also select Tenant. Add Display Name and Select a Tenant. Display Name: temp-stretch-01 Tenant setting: Tenant-01 3. Template to site association For configuration in template to be deployed, appropriate sites need to be added. Sites added will decide to which fabric configuration will be pushed. For temp-stretch-01 , we want to add both on-prem and AWS sites. To do it under the Template Properties locate the Actions button and hit Sites Associations Select both site on-prem and cAPIC and hit Ok VRF, EPG, selectors deployment 1. VRF Configuration Inside Schema-T01 , inside temp-stretch-01 add the VRF with \"Add VRF\" button under VRFs box Add VRF Common Properties Display Name: VRF-01 Description: CL2023 Stretch VRF Leave other as default. As Tempalte was assocaited to both sites, we need to update details for each of them, to do so under temp-stretch-01 expand \"Template Properties\" and go to \"cAPIC\" Click on the \"VRF-01\" which opens Site specific properties for this cAPIC Site Under the \"Template Properties\" hit \"Add Region\" button and select Region from the list and then hit \"Add CIDRs\" button Region: eu-west-2 Now we need to specify what subnet we would like to use in AWS cloud for that VPC. This subnet will be configure as VPC CIDR on AWS cloud and will be used for VM and endpoint addressing. CIDR: 10.0.0.0/23 Our CIDR also needs to be divided into subnets for respective Availability Zones (AZ) - hit \"Add Subnet\" to configure it. We need to add subnet for a,b,c (number of AZ depends on the region), each subet needs to be confirmed with click on checkbox button. Create subnet for all 3 AZ in our Region, use non-virtual zone. Subnet: 10.0.0.0/25 Name: az-2a-subnet AZ: eu-west-2a Subnet: 10.0.0.128/25 Name: az-2b-subnet AZ: eu-west-2b Subnet: 10.0.1.0/25 Name: az-2c-subnet AZ: eu-west-2c When done, hit \"Save\" button to save subnet configuration. We also want to connect our subnets to Catalys8000V routers, this is done via Hub Network (Transit Gateway in AWS) - check the checkbox for \"Hub Network\" , select Hub Network and also all add all Subnets created before. Hub Network: TGW-HUB Subnets: 10.0.0.0/25, 10.0.0.128/25, 10.0.1.0/25 Hit ok to finish configuration for cAPIC fabric. Info AWS Transit Gateway(TGW) connects Amazon Virtual Private Clouds (VPCs) networks through a central hub. This connection simplifies network and puts an end to complex peering relationships. Transit Gateway acts as a highly scalable cloud router(don't be confused by naming - it's AWS internal cloud service and it's not related to Catalyst8000V routers which we also used call Cloud Routers). In our case TGW are used to connect our User VPC with Infrastructure VPC where the Cloud Routers (Catalyst8000V) resides, this allows traffic flow between User Endpoints and Cloud Routes, so also for communication with another sites and also another AWS Regions. Selection of the subnet for Hub Network means that such subnet will be advertised to TGW, so traffic towards such subnet can flow. Under temp-stretch-01 expand \"Template Properties\" and go to \"Template Properties\" main settings. When done \"Deploy to sites\" button will become active (blue) - click to deploy VRF to respective sites. Nexus Dashboard will also show what changes are to be made. Review those and hit Deploy button to push configuration. If all goes well, confirmation pop-up will get displayed on the screen. At this point in both on-prem and AWS cloud, there will be VRF/VPC created. 2. VRF verification In the browser, open AWS console page. From the Region list, make sure that you have London (eu-west-2) region selected. If not, switch to it. Search for \"VPC\" resource in Search tool and open it. You should have 2 VPCs created. context-[VRF-01]-addr-[10.0.0.0/23] - coresponding to NDO created VRF default, without name, created automaticaly by AWS Let's check now VRF on on-prem APIC. Open APIC GUI and go to Tenants section. Double-click at Tenant-1 to enter configuration of your freshly created Tenant - using deploy at NDO. Go to Networking -> VRFs to check if new VRF-01 is created. As you can see, creation of new Tenant and VRF from NDO has been done successfully. Let's continue with next steps. 3. Additional Template creation As per our topology diagram, EPGs in each Site are separate. As current temp-stretch-01 is associated with both sites. We need to create two(2) new templates in the Schema-T01 schema, for on-prem and AWS cloud sites respectively. Use the \"Add new Template\" and create 2 new templates with following names. Similar to first template from Actions menu associate to respective sites. AWS Template: Template name: temp-AWS-01 Template type: ACI Multi-Cloud Template Tenant: Tenant-01 Site associated: cAPIC Hit Save button to save the template. on-prem Template: Template name: temp-on-prem-01 Template type: ACI Multi-Cloud Template Tenant: Tenant-01 Site associated: on-prem Hit Save button to save the template. 4. Application Profile and EPG Configuration for AWS ACI in Cloud doesn't use the concept of Bridge Domain , they don't have any representation in Cloud, that's why IP ranges were defined as part of the VRF configuration. EPGs can be now created and attached to VRF directly. Navigate to temp-AWS-01 template using View dropdown menu. Add Application Profile using Add Application Profile button. Display Name: AppProf-AWS-01 Description: Application Profile CL 2023 AWS Hit Save Under created Application Profile add EPG Display Name: EPG-AWS-01 Description: EPG AWS CL 2023 EPG Type: Application Skip the \"On-Premises Properties\" and click on \"Cloud Properties\" and select \"Virtual Routing & Forwarding\" as \"VRF-01\" . We also need to add Selector . Selectors are used in Public Cloud to assign Virtual Machines and Endpoints to correct EPG represented by a Security Group. Selector should be added Under Site Specific configuration for each EPG. Thare can be different type of selectores: IP based TAG based Region Based Custom Info Selectors should be designed in a way that's Instances/Service are matched by only one selector - different configuration will result in Fault. In our case we will use IP based selectors. Under Template Properties swtich to cAPIC Site, click on EPG-AWS-01 and hit \"Add Selector\" button under the EPG Cloud Properties. Selector details: Endpoint Selector Name: AWS-sel Expression: Type: IP address Operator: Equals Value: 10.0.0.0/23 Hit checkbox sign to save expression and then Ok to finish. Under Template Properties swtich back to Template Properties and hit \"Deploy to sites\" and \"Deploy\" . Once done confirmation will pop up on the screen. 5. Application Profile and EPG Configuration for on-prem APIC In onprem ACI deployment, EPG is still assigned to Bridge-Domain. Before you create EPG, please create new Bridge Domain. on-prem Template: Bridge Domain name: BD-01 VRF Name: VRF-01 Gateway IP: 172.16.255.1/24 Navigate to temp-on-prem-01 template using View dropdown menu. Scroll down to Brdige Domain section and click on Add Bridge Domain To add Gateway IP, scroll down right configuration section. You will find Gateway IP, click Add Subnet . Add new Subnet, leave rest or properties as is. When Bridge Domain BD-01 is ready, Continue with Application profile and EPG. Navigate to temp-on-prem-01 template using View dropdown menu. Add Application Profile using Add Application Profile button. Display Name: AppProf-01 Description: Application Profile CL 2023 Hit Save Under created Application Profile add EPG Display Name: EPG-on-prem-01 Description: EPG CL 2023 EPG Type: Application Configure the \"On-Premises Properties\" and select \"Bridge Domain\" as \"BD-01\" . Hit \"Deploy to sites\" and \"Deploy\" . Once done confirmation will pop up on the screen. At this point we have created Tenant, VRF, BD EPGs and selectors - let's now add security contracts. Contract configuration 1. Filter creation In order to create Contract, we need to have a filter which defines what type of traffis is allowed or denied under specific contract subject. Filter can define what ports and protocols are matched by specific rule. Open Nexus Dashboard Orchestrator GUI then go to Application Management -> Schemas -> \"Schema-T01\" -> open Under View select \"temp-stretch-01\" and \"Add filter\" under Filter section. Display Name: permit-any Then click \"Add entry\" to define protocols and ports. Name: permit-any Leave rest setting as default - this will allow for all protocols and ports, but we could limit traffic to very specific conditions. Hit Ok to save it. 2. Contract configuration As contracts will be used to connect AWS EPG to on-prem EPG, contract should be created in stretched template so it's deployed in both sites. Under View select \"temp-stretch-01\" and \"Add contract\" under Contract section. Create two(2) contracts with following details: Contract con-AWS-01-to-onprem-01 Display Name: con-AWS-01-to-onprem-01 Scope: VRF Apply both direction: yes Add Filter: permit-any Hit Save to finish contract configuration. Contract con-onprem-01-to-AWS-01 Display Name: con-onprem-01-to-AWS-01 Scope: VRF Apply both direction: yes Add Filter: permit-any Hit Save to finish contract configuration. Hit Deploy to sites for contracts to be pushed. Info Even the checkbox for \"Apply both direction\" is enabled, we still need 2 contracts between Cloud EPGs. This configuration is coming from the fact that Consumed Contracts are adding Outbound/Outgoing rules and Provided Contracts are adding Inband/Incoming rules, so with one contract we would only allow for traffic to be initiated from consumer EPG. We could use the same contract in both direction (each EPG would then provide and consume the same contract), but it's not a best practice as may lead to unwanted traffic leaking and complicate troubleshooting. 2. Contract assigment to EPGs Just creation of contract doesn't have any impact on the traffic. Contract needs to have at least one Provider EPG and one Consumer EPG to allow communication between them. Under View select \"temp-AWS-01\" click on the EPG-AWS-01 and under EPG specific setting locate Contract section Hit \"Add Contract\" button and add the following Contract 1: EPG: EPG-AWS-01 Contract: con-AWS-01-to-onprem-01 Type: provider Contract 2: EPG: EPG-AWS-01 Contract: con-onprem-01-to-AWS-01 Type: consumer Hit Deploy to sites for contracts to be applied to EPGs. Under View select \"temp-on-prem-01\" click on the EPG-on-prem-01 and under EPG specific setting locate Contract section Hit \"Add Contract\" button and add the following Contract 1: EPG: EPG-on-prem-01 Contract: con-onprem-01-to-AWS-01 Type: provider Contract 2: EPG: EPG-on-prem-01 Contract: con-AWS-01-to-onprem-01 Type: consumer Hit Deploy to sites for contracts to be applied to EPGs. EC2/VM creation and verification Next step is creation of AWS EC2 Instance for EPG associacion verification. 1. AWS EC2 creation Login to AWS User tenant via https://console.aws.amazon.com and make sure that you have London/eu-west-2 region selected. To be able to launch and login to VM we need to have SSH key-pair created. In the search bar type \"key pairs\" and select Key Pairs from Features list. Hit \"Create key pair\" in top right corner and provide following settings: Name: AWS-key-pair Key pair type: RSA Private key file format: .pem Hit \"Create key pair\" to finish. Once it's done key will be downloaded to your workstation. In the search bar type \"EC2\" and select EC2 from Service list. On the EC2 Dashboard locate \"Launch instance\" and hit \"Launch instance\" option. Instance details: Name: VM-AWS-01 Application and OS Images: Amazon Linux Architecture: 64-bit Instance type: t2.micro Key pair(login): AWS-key-pair Network settings (hit Edit to change): VPC: context-[VRF-01]-addr[10.0.0.0/23] subnet: subnet-[10.0.0.0/25] Auto-assign public IP: Enable Firewall (security groups): leave default Other setting leave default Review summary and hit \"Launch instance\" to create Virtual Machine It may take 3-5 minutes for instance to be ready. 2. AWS EC2 verification Go back to EC2 -> Instances and locate your EC2 machine. Click on Instance-ID of you machine to open it. Verify that VPC ID is correct Verify that subnet is correct Verify that instance has correct Private IPv4 addressess\" Note down IP address of EC2 instance, we will need it for verification. Go to Security tab settings of VM and check that Security groups for you instance is the one related to EPG-AWS-01 Info This security group was created when EPG configuration was deployed towards AWS site. Instance interfaces were assigned to this EPG/Security Group based on the IP based Selector configured for this EPG. Cloud Network Controller is monitoring resources created in managed Tenants and it's automatically assigning Security groupes based on selectors. Note After EC2 instance startup it may take some time for CNC to change security group to correct one. If no selector is matched, EC2 will be assigned to default Cloud Network Controller Secuirty Group will all traffic denied.","title":"Use-case 01 - Stretched VRF"},{"location":"use-case1.html#use-case-01-stretched-vrf-with-epg-to-epg-communication","text":"First use-case task will focus on: Configuration of stretched VRF inside Tenant-01 VPC CIDR configuration in AWS EPG configuration in both sides EC2 creation and EPG assigment Contract configuration to allow traffic flow","title":"Use-case 01 - Stretched VRF with EPG to EPG communication"},{"location":"use-case1.html#schema-template-configuration","text":"All logical polices and configuration are done in Nexus Dashboard Orchestratod via Schemas and Templates. Each Schema can have multiple Templates, but Template can belong to only one Schema. Each Template is also assocaited to one and only one Tenant. Template to site association will also define to which fabric (on prem or cloud) configration will be pushed, therefore is the smallest logical unit you can decide where changes are deployed.","title":"Schema, Template configuration"},{"location":"use-case1.html#1-schema-creation","text":"Navigate to Dashboard -> Application Management -> Schemas , then hit \"Add Schema\" Add Schema name and Description and hit \"Add\" button Name: Schema-T01 Description: Schema for Tenant-01","title":"1. Schema creation"},{"location":"use-case1.html#2-template-creation","text":"Under the Schema-T01, let's create first Template with \"Add New Template\" button For a Template type select \"ACI Multi-Cloud\" and hit \"Add\" On the right side of the template screen, we can customize the Tempalte Display Name, and also select Tenant. Add Display Name and Select a Tenant. Display Name: temp-stretch-01 Tenant setting: Tenant-01","title":"2. Template creation"},{"location":"use-case1.html#3-template-to-site-association","text":"For configuration in template to be deployed, appropriate sites need to be added. Sites added will decide to which fabric configuration will be pushed. For temp-stretch-01 , we want to add both on-prem and AWS sites. To do it under the Template Properties locate the Actions button and hit Sites Associations Select both site on-prem and cAPIC and hit Ok","title":"3. Template to site association"},{"location":"use-case1.html#vrf-epg-selectors-deployment","text":"","title":"VRF, EPG, selectors deployment"},{"location":"use-case1.html#1-vrf-configuration","text":"Inside Schema-T01 , inside temp-stretch-01 add the VRF with \"Add VRF\" button under VRFs box Add VRF Common Properties Display Name: VRF-01 Description: CL2023 Stretch VRF Leave other as default. As Tempalte was assocaited to both sites, we need to update details for each of them, to do so under temp-stretch-01 expand \"Template Properties\" and go to \"cAPIC\" Click on the \"VRF-01\" which opens Site specific properties for this cAPIC Site Under the \"Template Properties\" hit \"Add Region\" button and select Region from the list and then hit \"Add CIDRs\" button Region: eu-west-2 Now we need to specify what subnet we would like to use in AWS cloud for that VPC. This subnet will be configure as VPC CIDR on AWS cloud and will be used for VM and endpoint addressing. CIDR: 10.0.0.0/23 Our CIDR also needs to be divided into subnets for respective Availability Zones (AZ) - hit \"Add Subnet\" to configure it. We need to add subnet for a,b,c (number of AZ depends on the region), each subet needs to be confirmed with click on checkbox button. Create subnet for all 3 AZ in our Region, use non-virtual zone. Subnet: 10.0.0.0/25 Name: az-2a-subnet AZ: eu-west-2a Subnet: 10.0.0.128/25 Name: az-2b-subnet AZ: eu-west-2b Subnet: 10.0.1.0/25 Name: az-2c-subnet AZ: eu-west-2c When done, hit \"Save\" button to save subnet configuration. We also want to connect our subnets to Catalys8000V routers, this is done via Hub Network (Transit Gateway in AWS) - check the checkbox for \"Hub Network\" , select Hub Network and also all add all Subnets created before. Hub Network: TGW-HUB Subnets: 10.0.0.0/25, 10.0.0.128/25, 10.0.1.0/25 Hit ok to finish configuration for cAPIC fabric. Info AWS Transit Gateway(TGW) connects Amazon Virtual Private Clouds (VPCs) networks through a central hub. This connection simplifies network and puts an end to complex peering relationships. Transit Gateway acts as a highly scalable cloud router(don't be confused by naming - it's AWS internal cloud service and it's not related to Catalyst8000V routers which we also used call Cloud Routers). In our case TGW are used to connect our User VPC with Infrastructure VPC where the Cloud Routers (Catalyst8000V) resides, this allows traffic flow between User Endpoints and Cloud Routes, so also for communication with another sites and also another AWS Regions. Selection of the subnet for Hub Network means that such subnet will be advertised to TGW, so traffic towards such subnet can flow. Under temp-stretch-01 expand \"Template Properties\" and go to \"Template Properties\" main settings. When done \"Deploy to sites\" button will become active (blue) - click to deploy VRF to respective sites. Nexus Dashboard will also show what changes are to be made. Review those and hit Deploy button to push configuration. If all goes well, confirmation pop-up will get displayed on the screen. At this point in both on-prem and AWS cloud, there will be VRF/VPC created.","title":"1. VRF Configuration"},{"location":"use-case1.html#2-vrf-verification","text":"In the browser, open AWS console page. From the Region list, make sure that you have London (eu-west-2) region selected. If not, switch to it. Search for \"VPC\" resource in Search tool and open it. You should have 2 VPCs created. context-[VRF-01]-addr-[10.0.0.0/23] - coresponding to NDO created VRF default, without name, created automaticaly by AWS Let's check now VRF on on-prem APIC. Open APIC GUI and go to Tenants section. Double-click at Tenant-1 to enter configuration of your freshly created Tenant - using deploy at NDO. Go to Networking -> VRFs to check if new VRF-01 is created. As you can see, creation of new Tenant and VRF from NDO has been done successfully. Let's continue with next steps.","title":"2. VRF verification"},{"location":"use-case1.html#3-additional-template-creation","text":"As per our topology diagram, EPGs in each Site are separate. As current temp-stretch-01 is associated with both sites. We need to create two(2) new templates in the Schema-T01 schema, for on-prem and AWS cloud sites respectively. Use the \"Add new Template\" and create 2 new templates with following names. Similar to first template from Actions menu associate to respective sites. AWS Template: Template name: temp-AWS-01 Template type: ACI Multi-Cloud Template Tenant: Tenant-01 Site associated: cAPIC Hit Save button to save the template. on-prem Template: Template name: temp-on-prem-01 Template type: ACI Multi-Cloud Template Tenant: Tenant-01 Site associated: on-prem Hit Save button to save the template.","title":"3. Additional Template creation"},{"location":"use-case1.html#4-application-profile-and-epg-configuration-for-aws","text":"ACI in Cloud doesn't use the concept of Bridge Domain , they don't have any representation in Cloud, that's why IP ranges were defined as part of the VRF configuration. EPGs can be now created and attached to VRF directly. Navigate to temp-AWS-01 template using View dropdown menu. Add Application Profile using Add Application Profile button. Display Name: AppProf-AWS-01 Description: Application Profile CL 2023 AWS Hit Save Under created Application Profile add EPG Display Name: EPG-AWS-01 Description: EPG AWS CL 2023 EPG Type: Application Skip the \"On-Premises Properties\" and click on \"Cloud Properties\" and select \"Virtual Routing & Forwarding\" as \"VRF-01\" . We also need to add Selector . Selectors are used in Public Cloud to assign Virtual Machines and Endpoints to correct EPG represented by a Security Group. Selector should be added Under Site Specific configuration for each EPG. Thare can be different type of selectores: IP based TAG based Region Based Custom Info Selectors should be designed in a way that's Instances/Service are matched by only one selector - different configuration will result in Fault. In our case we will use IP based selectors. Under Template Properties swtich to cAPIC Site, click on EPG-AWS-01 and hit \"Add Selector\" button under the EPG Cloud Properties. Selector details: Endpoint Selector Name: AWS-sel Expression: Type: IP address Operator: Equals Value: 10.0.0.0/23 Hit checkbox sign to save expression and then Ok to finish. Under Template Properties swtich back to Template Properties and hit \"Deploy to sites\" and \"Deploy\" . Once done confirmation will pop up on the screen.","title":"4. Application Profile and EPG Configuration for AWS"},{"location":"use-case1.html#5-application-profile-and-epg-configuration-for-on-prem-apic","text":"In onprem ACI deployment, EPG is still assigned to Bridge-Domain. Before you create EPG, please create new Bridge Domain. on-prem Template: Bridge Domain name: BD-01 VRF Name: VRF-01 Gateway IP: 172.16.255.1/24 Navigate to temp-on-prem-01 template using View dropdown menu. Scroll down to Brdige Domain section and click on Add Bridge Domain To add Gateway IP, scroll down right configuration section. You will find Gateway IP, click Add Subnet . Add new Subnet, leave rest or properties as is. When Bridge Domain BD-01 is ready, Continue with Application profile and EPG. Navigate to temp-on-prem-01 template using View dropdown menu. Add Application Profile using Add Application Profile button. Display Name: AppProf-01 Description: Application Profile CL 2023 Hit Save Under created Application Profile add EPG Display Name: EPG-on-prem-01 Description: EPG CL 2023 EPG Type: Application Configure the \"On-Premises Properties\" and select \"Bridge Domain\" as \"BD-01\" . Hit \"Deploy to sites\" and \"Deploy\" . Once done confirmation will pop up on the screen. At this point we have created Tenant, VRF, BD EPGs and selectors - let's now add security contracts.","title":"5. Application Profile and EPG Configuration for on-prem APIC"},{"location":"use-case1.html#contract-configuration","text":"","title":"Contract configuration"},{"location":"use-case1.html#1-filter-creation","text":"In order to create Contract, we need to have a filter which defines what type of traffis is allowed or denied under specific contract subject. Filter can define what ports and protocols are matched by specific rule. Open Nexus Dashboard Orchestrator GUI then go to Application Management -> Schemas -> \"Schema-T01\" -> open Under View select \"temp-stretch-01\" and \"Add filter\" under Filter section. Display Name: permit-any Then click \"Add entry\" to define protocols and ports. Name: permit-any Leave rest setting as default - this will allow for all protocols and ports, but we could limit traffic to very specific conditions. Hit Ok to save it.","title":"1. Filter creation"},{"location":"use-case1.html#2-contract-configuration","text":"As contracts will be used to connect AWS EPG to on-prem EPG, contract should be created in stretched template so it's deployed in both sites. Under View select \"temp-stretch-01\" and \"Add contract\" under Contract section. Create two(2) contracts with following details: Contract con-AWS-01-to-onprem-01 Display Name: con-AWS-01-to-onprem-01 Scope: VRF Apply both direction: yes Add Filter: permit-any Hit Save to finish contract configuration. Contract con-onprem-01-to-AWS-01 Display Name: con-onprem-01-to-AWS-01 Scope: VRF Apply both direction: yes Add Filter: permit-any Hit Save to finish contract configuration. Hit Deploy to sites for contracts to be pushed. Info Even the checkbox for \"Apply both direction\" is enabled, we still need 2 contracts between Cloud EPGs. This configuration is coming from the fact that Consumed Contracts are adding Outbound/Outgoing rules and Provided Contracts are adding Inband/Incoming rules, so with one contract we would only allow for traffic to be initiated from consumer EPG. We could use the same contract in both direction (each EPG would then provide and consume the same contract), but it's not a best practice as may lead to unwanted traffic leaking and complicate troubleshooting.","title":"2. Contract configuration"},{"location":"use-case1.html#2-contract-assigment-to-epgs","text":"Just creation of contract doesn't have any impact on the traffic. Contract needs to have at least one Provider EPG and one Consumer EPG to allow communication between them. Under View select \"temp-AWS-01\" click on the EPG-AWS-01 and under EPG specific setting locate Contract section Hit \"Add Contract\" button and add the following Contract 1: EPG: EPG-AWS-01 Contract: con-AWS-01-to-onprem-01 Type: provider Contract 2: EPG: EPG-AWS-01 Contract: con-onprem-01-to-AWS-01 Type: consumer Hit Deploy to sites for contracts to be applied to EPGs. Under View select \"temp-on-prem-01\" click on the EPG-on-prem-01 and under EPG specific setting locate Contract section Hit \"Add Contract\" button and add the following Contract 1: EPG: EPG-on-prem-01 Contract: con-onprem-01-to-AWS-01 Type: provider Contract 2: EPG: EPG-on-prem-01 Contract: con-AWS-01-to-onprem-01 Type: consumer Hit Deploy to sites for contracts to be applied to EPGs.","title":"2. Contract assigment to EPGs"},{"location":"use-case1.html#ec2vm-creation-and-verification","text":"Next step is creation of AWS EC2 Instance for EPG associacion verification.","title":"EC2/VM creation and verification"},{"location":"use-case1.html#1-aws-ec2-creation","text":"Login to AWS User tenant via https://console.aws.amazon.com and make sure that you have London/eu-west-2 region selected. To be able to launch and login to VM we need to have SSH key-pair created. In the search bar type \"key pairs\" and select Key Pairs from Features list. Hit \"Create key pair\" in top right corner and provide following settings: Name: AWS-key-pair Key pair type: RSA Private key file format: .pem Hit \"Create key pair\" to finish. Once it's done key will be downloaded to your workstation. In the search bar type \"EC2\" and select EC2 from Service list. On the EC2 Dashboard locate \"Launch instance\" and hit \"Launch instance\" option. Instance details: Name: VM-AWS-01 Application and OS Images: Amazon Linux Architecture: 64-bit Instance type: t2.micro Key pair(login): AWS-key-pair Network settings (hit Edit to change): VPC: context-[VRF-01]-addr[10.0.0.0/23] subnet: subnet-[10.0.0.0/25] Auto-assign public IP: Enable Firewall (security groups): leave default Other setting leave default Review summary and hit \"Launch instance\" to create Virtual Machine It may take 3-5 minutes for instance to be ready.","title":"1. AWS EC2 creation"},{"location":"use-case1.html#2-aws-ec2-verification","text":"Go back to EC2 -> Instances and locate your EC2 machine. Click on Instance-ID of you machine to open it. Verify that VPC ID is correct Verify that subnet is correct Verify that instance has correct Private IPv4 addressess\" Note down IP address of EC2 instance, we will need it for verification. Go to Security tab settings of VM and check that Security groups for you instance is the one related to EPG-AWS-01 Info This security group was created when EPG configuration was deployed towards AWS site. Instance interfaces were assigned to this EPG/Security Group based on the IP based Selector configured for this EPG. Cloud Network Controller is monitoring resources created in managed Tenants and it's automatically assigning Security groupes based on selectors. Note After EC2 instance startup it may take some time for CNC to change security group to correct one. If no selector is matched, EC2 will be assigned to default Cloud Network Controller Secuirty Group will all traffic denied.","title":"2. AWS EC2 verification"},{"location":"use-case2.html","text":"Use-case 02 - Internet Gateway In this task we will configure Internet Gateway in AWS cloud, so EC2 from EPG-AWS-01 are able to reach the Internet. Instances in AWS can access Internet in AWS cloud with help of Internet Gateway. An internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between VPC and the internet. An internet gateway provides a target VPC route tables for internet-routable traffic. For communication using IPv4, the internet gateway also performs network address translation (NAT). Internet Gateway as External EPG As all VPC aspects and routing are controller by Cloud Network Controller, also Internet Gateway(IGW) is configured with it's help. Internet Gateway is represented by well know ACI object - External EPG. External EPG configuration Open Nexus Dashboard Orchestrator GUI then go to Application Management -> Schemas -> \"Schema-T01\" -> open Under View select \"temp-AWS-01\" and hit \"Add External EPGs\" under External EPGs section. Display Name: ExtEPG-IGW Description: Ext EPG for Internet Gateway AWS Virutal Routing & Forwarding: VRF-01 Select Site Type: CLOUD Cloud Properties: Application Profile: AppProf-AWS-01 Add Selector: Endpoint Selector Name: Internet Expression Type: IP Address Expression Operator: Equal Expression Value: 0.0.0.0/0 Hit Save to finish. Info Adding the \"0.0.0.0/0\" subnet will add additional route to this prefix, pointing to internet gateway object create in AWS. In case only specific prefixes should be rechable, you can use differnt subnet that 0.0.0.0/0. Contract configuration As External EPG is functioning as any other EPG, contract configuration from EPG to External EPG is mandatory for traffic to be allowed. 1. Filter creation Open Nexus Dashboard Orchestrator GUI then go to Application Management -> Schemas -> \"Schema-T01\" -> open Under View select \"temp-AWS-01\"\" and \"Add filter\" under Filter section. Display Name: permit-any-aws Then click \"Add entry\" to define protocols and ports. Name: permit-any Leave rest setting as default - this will allow for all protocols and ports. Hit Ok to save it. 2. Contract configuration As contracts will be used to connect AWS EPG to ExtEPG of AWS, contract should be created in AWS template Under View select \"temp-AWS-01\" and \"Add contract\" under Contract section. Create two(2) contracts with following details: Contract con-AWS-01-to-IGW Display Name: con-AWS-01-to-IGW Scope: VRF Apply both direction: yes Add Filter: permit-any-AWS Hit Save to finish contract configuration. Contract con-IGW-to-AWS-01 Display Name: con-IGW-to-AWS-01 Scope: VRF Apply both direction: yes Add Filter: permit-any-AWS Hit Save to finish contract configuration. Hit Deploy to sites for contracts to be pushed. Info Having contract only in one direction where Internet Gateway is a provider will be enough for EC2 instances to reach Internet, however contract in opposite direction is needed in case you want to connect to EC2 instances Public IPv4 Addresses from Internet. 3. Contract assigment to EPGs Under View select \"temp-AWS-01\" click on the EPG-AWS-01 and under EPG specific setting locate Contract section Hit \"Add Contract\" button and add the following Contract 1: EPG: EPG-AWS-01 Contract: con-AWS-01-to-IGW * Type: provider Contract 2: EPG: EPG-AWS-01 Contract: con-IGW-to-AWS-01 Type: consumer Under View select \"temp-AWS-01\" click on the ExtEPG-IGW and under Common Properties locate Contract section Hit \"Add Contract\" button and add the following Contract 1: EPG: ExtEPG-IGW Contract: con-IGW-to-AWS-01 Type: provider Contract 2: EPG: ExtEPG-IGW Contract: con-AWS-01-to-IGW Type: consumer Hit Deploy to sites to create IGW and deploy contracts associations. Verification Let's check if AWS EC2 instance is now reachable from internet. 1. In AWS Search Bar type \"EC2\" and Select \"EC2\" Services Tab 2. From \"Resources\" select \"Instances (Running)\" Locate Public IPv4 Address of EC2 instance, either on the EC2 instance list or under Details section From workstation open command line interface by hitting \"Start\" and typing \"cmd\" Execute command ping <Public IPv4 address of EC2> Communication is successfull now, EC2 instance has access to Internet via AWS Internet Gateway. At this point we completed that part of our topology configuration. You successfully completed Walk in Lab scenarious.","title":"Use-case 02 - Internet Gateway"},{"location":"use-case2.html#use-case-02-internet-gateway","text":"In this task we will configure Internet Gateway in AWS cloud, so EC2 from EPG-AWS-01 are able to reach the Internet. Instances in AWS can access Internet in AWS cloud with help of Internet Gateway. An internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between VPC and the internet. An internet gateway provides a target VPC route tables for internet-routable traffic. For communication using IPv4, the internet gateway also performs network address translation (NAT).","title":"Use-case 02 - Internet Gateway"},{"location":"use-case2.html#internet-gateway-as-external-epg","text":"As all VPC aspects and routing are controller by Cloud Network Controller, also Internet Gateway(IGW) is configured with it's help. Internet Gateway is represented by well know ACI object - External EPG.","title":"Internet Gateway as External EPG"},{"location":"use-case2.html#external-epg-configuration","text":"Open Nexus Dashboard Orchestrator GUI then go to Application Management -> Schemas -> \"Schema-T01\" -> open Under View select \"temp-AWS-01\" and hit \"Add External EPGs\" under External EPGs section. Display Name: ExtEPG-IGW Description: Ext EPG for Internet Gateway AWS Virutal Routing & Forwarding: VRF-01 Select Site Type: CLOUD Cloud Properties: Application Profile: AppProf-AWS-01 Add Selector: Endpoint Selector Name: Internet Expression Type: IP Address Expression Operator: Equal Expression Value: 0.0.0.0/0 Hit Save to finish. Info Adding the \"0.0.0.0/0\" subnet will add additional route to this prefix, pointing to internet gateway object create in AWS. In case only specific prefixes should be rechable, you can use differnt subnet that 0.0.0.0/0.","title":"External EPG configuration"},{"location":"use-case2.html#contract-configuration","text":"As External EPG is functioning as any other EPG, contract configuration from EPG to External EPG is mandatory for traffic to be allowed.","title":"Contract configuration"},{"location":"use-case2.html#1-filter-creation","text":"Open Nexus Dashboard Orchestrator GUI then go to Application Management -> Schemas -> \"Schema-T01\" -> open Under View select \"temp-AWS-01\"\" and \"Add filter\" under Filter section. Display Name: permit-any-aws Then click \"Add entry\" to define protocols and ports. Name: permit-any Leave rest setting as default - this will allow for all protocols and ports. Hit Ok to save it.","title":"1. Filter creation"},{"location":"use-case2.html#2-contract-configuration","text":"As contracts will be used to connect AWS EPG to ExtEPG of AWS, contract should be created in AWS template Under View select \"temp-AWS-01\" and \"Add contract\" under Contract section. Create two(2) contracts with following details: Contract con-AWS-01-to-IGW Display Name: con-AWS-01-to-IGW Scope: VRF Apply both direction: yes Add Filter: permit-any-AWS Hit Save to finish contract configuration. Contract con-IGW-to-AWS-01 Display Name: con-IGW-to-AWS-01 Scope: VRF Apply both direction: yes Add Filter: permit-any-AWS Hit Save to finish contract configuration. Hit Deploy to sites for contracts to be pushed. Info Having contract only in one direction where Internet Gateway is a provider will be enough for EC2 instances to reach Internet, however contract in opposite direction is needed in case you want to connect to EC2 instances Public IPv4 Addresses from Internet.","title":"2. Contract configuration"},{"location":"use-case2.html#3-contract-assigment-to-epgs","text":"Under View select \"temp-AWS-01\" click on the EPG-AWS-01 and under EPG specific setting locate Contract section Hit \"Add Contract\" button and add the following Contract 1: EPG: EPG-AWS-01 Contract: con-AWS-01-to-IGW * Type: provider Contract 2: EPG: EPG-AWS-01 Contract: con-IGW-to-AWS-01 Type: consumer Under View select \"temp-AWS-01\" click on the ExtEPG-IGW and under Common Properties locate Contract section Hit \"Add Contract\" button and add the following Contract 1: EPG: ExtEPG-IGW Contract: con-IGW-to-AWS-01 Type: provider Contract 2: EPG: ExtEPG-IGW Contract: con-AWS-01-to-IGW Type: consumer Hit Deploy to sites to create IGW and deploy contracts associations.","title":"3. Contract assigment to EPGs"},{"location":"use-case2.html#verification","text":"Let's check if AWS EC2 instance is now reachable from internet.","title":"Verification"},{"location":"use-case2.html#1-in-aws-search-bar-type-ec2-and-select-ec2-services-tab","text":"","title":"1. In AWS Search Bar type \"EC2\" and Select \"EC2\" Services Tab"},{"location":"use-case2.html#2-from-resources-select-instances-running","text":"Locate Public IPv4 Address of EC2 instance, either on the EC2 instance list or under Details section From workstation open command line interface by hitting \"Start\" and typing \"cmd\" Execute command ping <Public IPv4 address of EC2> Communication is successfull now, EC2 instance has access to Internet via AWS Internet Gateway. At this point we completed that part of our topology configuration. You successfully completed Walk in Lab scenarious.","title":"2. From \"Resources\" select \"Instances (Running)\""},{"location":"use-case3.html","text":"Use-case 03 - Inter-Tenant routing In this section of the lab we will expolore possibilites of Inter-tenant traffic flows. It's a very common scenarios where multiple accounts/tenants in Public Cloud need to communicate with one central one where common service are located. Tenant creation One additional Tenant needs to be created for this ucs-case configuration. Azure only tenant will be created, to avoid another trust configuration. On the Left navigation page click \"Application Management\" -> \"Tenant\" and then \"Add Tenant\" Fill in Tenant details for name and description Display Name: Tenant-Azure-02 Descrption: CL23 Tenant-02 Azure Associate Tenant to Azure CNC-Azure-01 Sites by checking the checkbox next to it. Note Similar as for first Tenant, we are not able to Save this configuration with red marking Site. Click the Pencil button at the end of each site line to complete configuration. Additional setting are needed for CNC, so it knows which subscribtion. CNC-Azure-01 site configuration For Azure site, we will be using the same Subscription as previously - select Mode as \"Select Shared\" and use existing subscription from drop-down. Leave security domains empty. Hit Save . As configuration will be done for new Tenant, new Schema, Template, VRF and all other logical objects have to be created. Schema, Template configuration 1. Schema creation Navigate to Dashboard -> Application Management -> Schemas , then hit \"Add Schema\" Add Schema name and Description and hit \"Add\" button Name: Schema-T02-Azure Description: Schema for Tenant-Azure-02 Let's now create template inside this schema. 2. Template creation Under the Schema-T02-Azure, let's create first Template with \"Add New Template\" button For a Template type select \"ACI Multi-Cloud\" and hit \"Add\" On the right side of the template screen, we can customize the Tempalte Display Name, and also select Tenant. Add Display Name and Select a Tenant. Display Name: temp-Azure-01 Tenant setting: Tenant-02-Azure 3. Template to site association For configuration in template to be deployed, appropriate sites need to be added. Sites added will decide to which fabric configuration will be pushed. For temp-Azure-01 , we want to add both Azure site only. To do it under the Template Properties locate the Actions button and hit Sites Associations Select CNC-Azure-01 site and hit Ok VRF, EPG, selectors deployment 1. VRF Configuration Inside Schema-T02-Azure , inside temp-Azure-01 add the VRF with \"Add VRF\" button under VRFs box Add VRF Common Properties Display Name: T2-VRF-02 Description: CL2023 VRF-02 Leave other as default. As Tempalte was assocaited to Azure site, we need to update details for it, to do so under temp-stretch-01 expand \"Template Properties\" and go to \"CNC-Azure-01\" Under temp-Azure-01 expand \"Tempalate Properties\" and go to \"CNC-Azure-01\" Click on the \"T2-VRF-02\" which opens Site specific properties for this CNC-Azure-01 Site Under the \"Tempalate Properties\" hit \"Add Region\" button and select Region from the list and then hit \"Add CIDRs\" button Region: francecentral Now we need to specify what subnet we would like to use in AWS cloud for that VPC. CIDR: 10.200.0.0/23 Like previously we need to add subnet, which is whole CIDR. When done, hit \"Save\" button to save subnet configuration. For traffic to flow between the Tenants, \"VNET Peering\" feature with \"Default\" Hub Network needs to be selected as well. Hit ok to finish configuration for CNC-Azure-01 fabric. Under temp-Azure-01 expand \"Template Properties\" and go to \"Template Properties\" main settings. When done \"Deploy to sites\" button will become active (blue) - click to deploy VRF to respective sites. Nexus Dashboard will also show what changes are to be made. Review those and hit Deploy button to push configuration. If all goes well, confirmation pop-up will get displayed on the screen. 2. Application Profile and EPG Configuration for Azure Navigate to temp-Azure-01 template using View dropdown menu. Add Application Profile using Add Application Profile button. Display Name: AppProf-Azure-T2-01 Description: Application Profile CL 2023 Azure T02 Hit Save Under created Application Profile add EPG Display Name: EPG-Azure-02 Description: EPG Azure CL 2023 T02 EPG Type: Application Skip the \"On-Premises Properties\" and click on \"Cloud Properties\" and select \"Virtual Routing & Forwarding\" as \"T2-VRF-02\" . We also need to add Selector for this EPG. Selector should be added Under Site Specific configuration for each EPG. Under Template Properties swtich to CNC-Azure-01 Site, click on EPG-Azure-02 and hit \"Add Selector\" button under the EPG Cloud Properties. Selector details: Endpoint Selector Name: Azure-sel-02 Expression: Type: IP address Operator: Equals Value: 10.200.0.0/23 Hit checkbox sign to save expression and then Ok to finish. Under Template Properties swtich back to Template Properties and hit \"Deploy to sites\" and \"Deploy\" . Once done confirmation will pop up on the screen. At this point we have created Tenant, VRF, EPGs and selectors for our second Tenant - let's now add Virtual Machines instance. VM creation Creation of VM for Tenant-Azure-02 in Azure. Login to Azure portal via https://portal.azure.com with your account details Search for Virtual machine in search bar and open from Services list Under Create button select \"Azure virtual machine\" Virtual Machine details: Note If setting is not listed, leave default. Subscription: leave selected Resource Group: hit create new and add with name \"RG-CL23-T02, then hit OK\" Virtual Machine name: VM-AZ-02 Region: (Europe) France Central Authentication type: Password username: student password: CiscoLive2023! Hit \"Next: Disks >\" and accept all default values, hit \"Next: Networking >\" and configure: Virtual Network: T2-VRF-02 Subnet: az-subnet (10.200.0.0/23) Public IP: leave default Hit directly \"Review + Create\" and leave all other setting as default. Hit directly \"Create\" to deploy Virtual Machine. It may take 3-5 minutes for instance to be ready. Portal will notify you when done. Let's add contract to allow communication. Contract configuration 1. Filter creation In order to create Contract, we need to have a filter in this template also Open Nexus Dashboard Orchestrator GUI then go to Application Management -> Schemas -> \"Schema-T02-Azure\" -> open Under View select \"temp-Azure-01\" and \"Add filter\" under Filter section. Display Name: permit-any Then click \"Add entry\" to define protocols and ports. Name: permit-any Leave rest setting as default - this will allow for all protocols and ports, but we could limit traffic to very specific conditions. Hit Ok to save it. 2. Contracts configuration Info Contract can have a different scope (Application Profile, VRF, Tenant, Global). Depneds on the use case, where are the providers and consumers located we should choose appropriate scope. In first two use-case providers and consumer were located in the same VRF, hence default VRF Scope was enough to cover provider and consumer. In this Use-case, idea is to connect EPGs from different Tenants, hence scope for contract needs to be set to \"Global\" , as this is the only one which covers EPGs from different tenants. Also placement of contract is imporant. Important If contract is to be used for connection of EPGs from different Tenants/VRFs, contract needs to be deployed in the Tenant were Provider of this contract is configured. In our case contract where EPG-Azure-02 from Tenant-Azure-02 is to be a Provider will be created in \"Schema-T02-Azure\" and Tempalte \"temp-Azure-01\" . Contract in opposite direction where EPG-Azure-01 from Tenant-01 is to be a Provider will be created in \"Schema-T01\" and Tempalte \"temp-Azure-01\" . Contracts configuration under Tenant-Azure-02 Under Schema \"Schema-T02-Azure\" navigate to View select \"temp-Azure-01\" and \"Add contract\" under Contract section. Create contracts with following details: Contract con-Azure-02-to-Azure-01 Display Name: con-Azure-02-to-Azure-01 Scope: Global Apply both direction: yes Add Filter: permit-any Hit Save to finish contract configuration. Under Schema \"Schema-T02-Azure\" navigate to View select \"temp-Azure-01\" and under EPG-Azure-02 specific setting locate Contract section Hit \"Add Contract\" button and add the following contract: EPG: EPG-Azure-02 Contract: con-Azure-02-to-Azure-01 Type: provider Hit Deploy to sites for contracts to be applied to EPGs. Let's now create contract in opposite direction. Under Schema \"Schema-T01\" navigate to View select \"temp-stretch-01\" and \"Add contract\" under Contract section. Create contracts with following details: Contract con-Azure-01-to-Azure-02 Display Name: con-Azure-01-to-Azure-02 Scope: Global Apply both direction: yes Add Filter: permit-any Under Schema \"Schema-T01\" navigate to View select \"temp-Azure-01\" and under EPG-Azure-01 specific setting locate Contract section Hit \"Add Contract\" button and add the following contract: Contract 1: EPG: EPG-Azure-01 Contract: con-Azure-01-to-Azure-02 Type: provider Contract 2: EPG: EPG-Azure-01 Contract: con-Azure-02-to-Azure-01 Type: consumer Hit Deploy to sites for contracts to be applied to EPGs. Let's come back to Tenant-Azure-02 to consume newly created contract. Under Schema \"Schema-T02-Azure\" navigate to View select \"temp-Azure-01\" and under EPG-Azure-02 specific setting locate Contract section Hit \"Add Contract\" button and add the following contract: EPG: EPG-Azure-02 Contract: con-Azure-01-to-Azure-02 Type: consumer Hit Deploy to sites for contracts to be applied to EPGs. ## Cross-Tenant traffic verification Let's check now if our Virtual Machines are able to communicate. For now we have configured that part of our infrastructure. Login to Azure portal via https://portal.azure.com with your account details Search for Virtual machine in search bar and open from Services list On the Azure Virtual Machine VM-AZ-02 , scroll down to \"Help\" Section and select \"Serial Console\" Hit enter and provide VM login credentials: username: student password: CiscoLive2023! Once in the console try to reach via ping to private IP address of Virutal Machine VM-AZ-01 student@VM-AZ-02:~$ ping 10.100.0.4 PING 10.100.0.4 (10.100.0.4) 56(84) bytes of data. 64 bytes from 10.100.0.4: icmp_seq=1 ttl=63 time=4.48 ms 64 bytes from 10.100.0.4: icmp_seq=2 ttl=63 time=6.70 ms 64 bytes from 10.100.0.4: icmp_seq=3 ttl=63 time=3.19 ms 64 bytes from 10.100.0.4: icmp_seq=4 ttl=63 time=6.22 ms ^C --- 10.100.0.4 ping statistics --- 4 packets transmitted, 4 received, 0% packet loss, time 3005ms rtt min/avg/max/mdev = 3.185/5.147/6.699/1.401 ms student@VM-AZ-02:~$ Communication is successfull! Well done. This test has completed our lab, hope you enjoyed it and learn some new infromations.","title":"Use-case 03 - Inter-Tenant routing"},{"location":"use-case3.html#use-case-03-inter-tenant-routing","text":"In this section of the lab we will expolore possibilites of Inter-tenant traffic flows. It's a very common scenarios where multiple accounts/tenants in Public Cloud need to communicate with one central one where common service are located.","title":"Use-case 03 - Inter-Tenant routing"},{"location":"use-case3.html#tenant-creation","text":"One additional Tenant needs to be created for this ucs-case configuration. Azure only tenant will be created, to avoid another trust configuration. On the Left navigation page click \"Application Management\" -> \"Tenant\" and then \"Add Tenant\" Fill in Tenant details for name and description Display Name: Tenant-Azure-02 Descrption: CL23 Tenant-02 Azure Associate Tenant to Azure CNC-Azure-01 Sites by checking the checkbox next to it. Note Similar as for first Tenant, we are not able to Save this configuration with red marking Site. Click the Pencil button at the end of each site line to complete configuration. Additional setting are needed for CNC, so it knows which subscribtion. CNC-Azure-01 site configuration For Azure site, we will be using the same Subscription as previously - select Mode as \"Select Shared\" and use existing subscription from drop-down. Leave security domains empty. Hit Save . As configuration will be done for new Tenant, new Schema, Template, VRF and all other logical objects have to be created.","title":"Tenant creation"},{"location":"use-case3.html#schema-template-configuration","text":"","title":"Schema, Template configuration"},{"location":"use-case3.html#1-schema-creation","text":"Navigate to Dashboard -> Application Management -> Schemas , then hit \"Add Schema\" Add Schema name and Description and hit \"Add\" button Name: Schema-T02-Azure Description: Schema for Tenant-Azure-02 Let's now create template inside this schema.","title":"1. Schema creation"},{"location":"use-case3.html#2-template-creation","text":"Under the Schema-T02-Azure, let's create first Template with \"Add New Template\" button For a Template type select \"ACI Multi-Cloud\" and hit \"Add\" On the right side of the template screen, we can customize the Tempalte Display Name, and also select Tenant. Add Display Name and Select a Tenant. Display Name: temp-Azure-01 Tenant setting: Tenant-02-Azure","title":"2. Template creation"},{"location":"use-case3.html#3-template-to-site-association","text":"For configuration in template to be deployed, appropriate sites need to be added. Sites added will decide to which fabric configuration will be pushed. For temp-Azure-01 , we want to add both Azure site only. To do it under the Template Properties locate the Actions button and hit Sites Associations Select CNC-Azure-01 site and hit Ok","title":"3. Template to site association"},{"location":"use-case3.html#vrf-epg-selectors-deployment","text":"","title":"VRF, EPG, selectors deployment"},{"location":"use-case3.html#1-vrf-configuration","text":"Inside Schema-T02-Azure , inside temp-Azure-01 add the VRF with \"Add VRF\" button under VRFs box Add VRF Common Properties Display Name: T2-VRF-02 Description: CL2023 VRF-02 Leave other as default. As Tempalte was assocaited to Azure site, we need to update details for it, to do so under temp-stretch-01 expand \"Template Properties\" and go to \"CNC-Azure-01\" Under temp-Azure-01 expand \"Tempalate Properties\" and go to \"CNC-Azure-01\" Click on the \"T2-VRF-02\" which opens Site specific properties for this CNC-Azure-01 Site Under the \"Tempalate Properties\" hit \"Add Region\" button and select Region from the list and then hit \"Add CIDRs\" button Region: francecentral Now we need to specify what subnet we would like to use in AWS cloud for that VPC. CIDR: 10.200.0.0/23 Like previously we need to add subnet, which is whole CIDR. When done, hit \"Save\" button to save subnet configuration. For traffic to flow between the Tenants, \"VNET Peering\" feature with \"Default\" Hub Network needs to be selected as well. Hit ok to finish configuration for CNC-Azure-01 fabric. Under temp-Azure-01 expand \"Template Properties\" and go to \"Template Properties\" main settings. When done \"Deploy to sites\" button will become active (blue) - click to deploy VRF to respective sites. Nexus Dashboard will also show what changes are to be made. Review those and hit Deploy button to push configuration. If all goes well, confirmation pop-up will get displayed on the screen.","title":"1. VRF Configuration"},{"location":"use-case3.html#2-application-profile-and-epg-configuration-for-azure","text":"Navigate to temp-Azure-01 template using View dropdown menu. Add Application Profile using Add Application Profile button. Display Name: AppProf-Azure-T2-01 Description: Application Profile CL 2023 Azure T02 Hit Save Under created Application Profile add EPG Display Name: EPG-Azure-02 Description: EPG Azure CL 2023 T02 EPG Type: Application Skip the \"On-Premises Properties\" and click on \"Cloud Properties\" and select \"Virtual Routing & Forwarding\" as \"T2-VRF-02\" . We also need to add Selector for this EPG. Selector should be added Under Site Specific configuration for each EPG. Under Template Properties swtich to CNC-Azure-01 Site, click on EPG-Azure-02 and hit \"Add Selector\" button under the EPG Cloud Properties. Selector details: Endpoint Selector Name: Azure-sel-02 Expression: Type: IP address Operator: Equals Value: 10.200.0.0/23 Hit checkbox sign to save expression and then Ok to finish. Under Template Properties swtich back to Template Properties and hit \"Deploy to sites\" and \"Deploy\" . Once done confirmation will pop up on the screen. At this point we have created Tenant, VRF, EPGs and selectors for our second Tenant - let's now add Virtual Machines instance.","title":"2. Application Profile and EPG Configuration for Azure"},{"location":"use-case3.html#vm-creation","text":"Creation of VM for Tenant-Azure-02 in Azure. Login to Azure portal via https://portal.azure.com with your account details Search for Virtual machine in search bar and open from Services list Under Create button select \"Azure virtual machine\" Virtual Machine details: Note If setting is not listed, leave default. Subscription: leave selected Resource Group: hit create new and add with name \"RG-CL23-T02, then hit OK\" Virtual Machine name: VM-AZ-02 Region: (Europe) France Central Authentication type: Password username: student password: CiscoLive2023! Hit \"Next: Disks >\" and accept all default values, hit \"Next: Networking >\" and configure: Virtual Network: T2-VRF-02 Subnet: az-subnet (10.200.0.0/23) Public IP: leave default Hit directly \"Review + Create\" and leave all other setting as default. Hit directly \"Create\" to deploy Virtual Machine. It may take 3-5 minutes for instance to be ready. Portal will notify you when done. Let's add contract to allow communication.","title":"VM creation"},{"location":"use-case3.html#contract-configuration","text":"","title":"Contract configuration"},{"location":"use-case3.html#1-filter-creation","text":"In order to create Contract, we need to have a filter in this template also Open Nexus Dashboard Orchestrator GUI then go to Application Management -> Schemas -> \"Schema-T02-Azure\" -> open Under View select \"temp-Azure-01\" and \"Add filter\" under Filter section. Display Name: permit-any Then click \"Add entry\" to define protocols and ports. Name: permit-any Leave rest setting as default - this will allow for all protocols and ports, but we could limit traffic to very specific conditions. Hit Ok to save it.","title":"1. Filter creation"},{"location":"use-case3.html#2-contracts-configuration","text":"Info Contract can have a different scope (Application Profile, VRF, Tenant, Global). Depneds on the use case, where are the providers and consumers located we should choose appropriate scope. In first two use-case providers and consumer were located in the same VRF, hence default VRF Scope was enough to cover provider and consumer. In this Use-case, idea is to connect EPGs from different Tenants, hence scope for contract needs to be set to \"Global\" , as this is the only one which covers EPGs from different tenants. Also placement of contract is imporant. Important If contract is to be used for connection of EPGs from different Tenants/VRFs, contract needs to be deployed in the Tenant were Provider of this contract is configured. In our case contract where EPG-Azure-02 from Tenant-Azure-02 is to be a Provider will be created in \"Schema-T02-Azure\" and Tempalte \"temp-Azure-01\" . Contract in opposite direction where EPG-Azure-01 from Tenant-01 is to be a Provider will be created in \"Schema-T01\" and Tempalte \"temp-Azure-01\" . Contracts configuration under Tenant-Azure-02 Under Schema \"Schema-T02-Azure\" navigate to View select \"temp-Azure-01\" and \"Add contract\" under Contract section. Create contracts with following details: Contract con-Azure-02-to-Azure-01 Display Name: con-Azure-02-to-Azure-01 Scope: Global Apply both direction: yes Add Filter: permit-any Hit Save to finish contract configuration. Under Schema \"Schema-T02-Azure\" navigate to View select \"temp-Azure-01\" and under EPG-Azure-02 specific setting locate Contract section Hit \"Add Contract\" button and add the following contract: EPG: EPG-Azure-02 Contract: con-Azure-02-to-Azure-01 Type: provider Hit Deploy to sites for contracts to be applied to EPGs. Let's now create contract in opposite direction. Under Schema \"Schema-T01\" navigate to View select \"temp-stretch-01\" and \"Add contract\" under Contract section. Create contracts with following details: Contract con-Azure-01-to-Azure-02 Display Name: con-Azure-01-to-Azure-02 Scope: Global Apply both direction: yes Add Filter: permit-any Under Schema \"Schema-T01\" navigate to View select \"temp-Azure-01\" and under EPG-Azure-01 specific setting locate Contract section Hit \"Add Contract\" button and add the following contract: Contract 1: EPG: EPG-Azure-01 Contract: con-Azure-01-to-Azure-02 Type: provider Contract 2: EPG: EPG-Azure-01 Contract: con-Azure-02-to-Azure-01 Type: consumer Hit Deploy to sites for contracts to be applied to EPGs. Let's come back to Tenant-Azure-02 to consume newly created contract. Under Schema \"Schema-T02-Azure\" navigate to View select \"temp-Azure-01\" and under EPG-Azure-02 specific setting locate Contract section Hit \"Add Contract\" button and add the following contract: EPG: EPG-Azure-02 Contract: con-Azure-01-to-Azure-02 Type: consumer Hit Deploy to sites for contracts to be applied to EPGs. ## Cross-Tenant traffic verification Let's check now if our Virtual Machines are able to communicate. For now we have configured that part of our infrastructure. Login to Azure portal via https://portal.azure.com with your account details Search for Virtual machine in search bar and open from Services list On the Azure Virtual Machine VM-AZ-02 , scroll down to \"Help\" Section and select \"Serial Console\" Hit enter and provide VM login credentials: username: student password: CiscoLive2023! Once in the console try to reach via ping to private IP address of Virutal Machine VM-AZ-01 student@VM-AZ-02:~$ ping 10.100.0.4 PING 10.100.0.4 (10.100.0.4) 56(84) bytes of data. 64 bytes from 10.100.0.4: icmp_seq=1 ttl=63 time=4.48 ms 64 bytes from 10.100.0.4: icmp_seq=2 ttl=63 time=6.70 ms 64 bytes from 10.100.0.4: icmp_seq=3 ttl=63 time=3.19 ms 64 bytes from 10.100.0.4: icmp_seq=4 ttl=63 time=6.22 ms ^C --- 10.100.0.4 ping statistics --- 4 packets transmitted, 4 received, 0% packet loss, time 3005ms rtt min/avg/max/mdev = 3.185/5.147/6.699/1.401 ms student@VM-AZ-02:~$ Communication is successfull! Well done.","title":"2. Contracts configuration"},{"location":"use-case3.html#this-test-has-completed-our-lab-hope-you-enjoyed-it-and-learn-some-new-infromations","text":"","title":"This test has completed our lab, hope you enjoyed it and learn some new infromations."}]}